{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Práctica  2. Deep Learning para clasificación\n",
        "---\n",
        "\n",
        "**Sistemas Inteligentes para la Gestión en la Empresa**\n",
        "\n",
        "Máster Profesional en Ingeniería Informática. Universidad de Granada.\n",
        "\n",
        "Curso académico 2022-2023\n",
        "\n",
        "---\n",
        "**Autores:**\n",
        "- Ramón García Verjaga (rgarver@correo.ugr.es)\n",
        "- José Alberto Gómez García (modej@correo.ugr.es)\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducción"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En los últimos años, el campo de la visión por computador ha experimentado un rápido avance gracias a los avances en el aprendizaje automático y, en particular, en las redes neuronales profundas. En esta segunda práctica de la asignatura \"Sistemas Inteligentes para la Gestión en la Empresa\" haremos uso de modelos de clasificación basados en redes neuronales profundas para abordar la clasificación de aves, en función de su especie, a partir de imágenes de las mismas.\n",
        "\n",
        "La clasificación de aves es una tarea desafiante debido a la gran variabilidad y similitudes entre las especies. El conjunto de datos CUB-200-2011, abreviatura de Caltech-UCSD Birds-200-2011, es una referencia comúnmente utilizada en la comunidad de visión por computador para abordar esta tarea y probar el desempeño de distintos modelos de clasificación. El conjunto original, disponible en la web [Caltech Library](https://data.caltech.edu/records/65de6-vp158) contiene más de 11.000 imágenes de 200 especies de aves diferentes, con una amplia diversidad de apariencias y posturas. Además del conjunto original, usaremos una versión reducida de este conjunto de datos, con algo más de 1.000 imágenes de 20 especies de aves distintas.\n",
        "\n",
        "Para desarrollar esta práctica haremos uso del lenguaje de programación Python, así como del framework para el desarrollo de redes neuronales profundas PyTorch. Adicionalmente, necesitaremos de otras librerías, como Numpy, Matplotlib y Scikit-Learn para almacenar datos, generar gráficas y realizar ciertos cálculos. Por tanto, para la ejecución de este cuaderno es necesario disponer de estos paquetes software. Si se ejecuta este cuaderno en Google Colab todos los requisitos están satisfechos; de ejecutarse en una máquina local deben ser instalados haciendo uso del comando `pip install -r requirements.txt`. Se recomienda hacer uso de un entorno Linux, pues ocasiona menos problemas para instalar y usar PyTorch con soporte con GPU, imprescindible si queremos entrenar redes neuronales en un tiempo asumible.\n",
        "\n",
        "Para que el cuaderno pueda ejecutarse sin ningún problema, el directorio que lo contiene (resultado de descomprimir el fichero entregado) deberá tener la siguiente estructura:\n",
        "\n",
        "- dataset/\n",
        "    - data additional/\n",
        "    - data x20/\n",
        "    - data x200/\n",
        "    - results/\n",
        "        - model_X_SOME_CHARACTERISTICS.pt\n",
        "        - model_Y_OTHER_CHARACTERISTICS.pt\n",
        "    - best_training_logs/\n",
        "        - model_X_METHOD_X.txt\n",
        "        - model_Y_METHOD_Y.txt \n",
        "- img_for_docs/\n",
        "- CODE_AND_REPORT.ipynb\n",
        "- requirements.txt\n",
        "\n",
        "Nótese que la estructura dentro de la carpeta `dataset` es la proporcionada en el fichero comprimido que se nos proporcionó con el conjunto de datos (con alguna consideración que discutiremos posteriormente). La carpeta `img_for_docs` simplemente contiene imágenes que se mostrarán durante este cuaderno. Dado el gran tamaño del conjunto de datos, no se adjuntan las carpetas `data x20` ni `data x200`, pero sí que se adjunta la carpeta `data additional`. El directorio `results` se generará automáticamente de no existir, pero en principio contendrá un subdirectorio con los logs impresos en la terminal durante la ejecución de los mejores entrenamientos, cuyos detalles no se han abordado en este cuaderno. Los ficheros con los modelos deben ser descargados, como se explicará más adelante. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k687if8hRzYN"
      },
      "source": [
        "## Configuración del entorno de trabajo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta sección nos centraremos fundamentalmente en preparar el entorno de ejecución del que haremos uso a lo largo de la práctica.\n",
        "\n",
        "En primer lugar definiremos 2 variables para determinar el contexto general de ejecución del programa.\n",
        "\n",
        "- `USING_COLAB` especifica si nos encontramos haciendo uso del servicio en la nube de Google Colab (con el que podemos acceder gratuitamente a GPUs) o si ejecutamos el software en local. \n",
        "Este servicio ha sido usado para poder desarrollar el cuaderno de forma conjunta, y se llegó a utilizar para realizar algunas ejecuciones. Dado que los tiempos de ejecución eran muy elevados y en ocasiones se producían bloqueos del programa y desconexiones, finalmente se realizaron las ejecuciones en el ordenador de José Alberto. Para contextualizar los tiempos de ejecución que se mostrarán posteriormente, este ordenador dispone de un procesador Intel Core i7 8700K, 32 GB de memoria RAM DDR4, un SSD Samsung 970 Evo Plus de 1 TB y una tarjeta gráfica NVIDIA GTX 1080 (8 GB de VRAM).\n",
        "- `EXECUTING_FOR_FIRST_TIME` es un flag que especifica si se deben ejecutar operaciones que solo deben ejecutarse una única vez, como comprobar la integridad de los datos, generar algunas gráficas en el marco del análisis exploratorio y demás. Si durante la corrección de esta práctica se ejecuta el cuaderno, se recomienda mantener su valor en `False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sN6JU1aPokbM"
      },
      "outputs": [],
      "source": [
        "USING_COLAB = False\n",
        "EXECUTING_FOR_FIRST_TIME = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Posteriormente, cargaremos todas las librerías necesarias. \n",
        "\n",
        "Como se mencionó en la introducción, necesitaremos de PyTorch, Numpy, Matplotlib y Scikit-Learn. Adicionalmente, se usan algunas librerías pre-instaladas de Python, como la necesaria para usar funcionalidades dependientes del sistema operativo, fundamentalmente para la gestión de ficheros y rutas (`os`), la librería de expresiones regulares (`re`), la librería para manejar aleatoriedad (`ramdom`) o la que habilita la paralelización de procesos en CPU (`multiprocessing`). De hacer uso de Google Colab, los datos deben ser cargados de Google Drive, por lo que neecistamos la librería que habilita la conexión entre ambos servicios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kGTXfkDvrSCl"
      },
      "outputs": [],
      "source": [
        "import os, re, random\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import torch.nn.functional as F\n",
        "import torchsummary\n",
        "import numpy as np\n",
        "import multiprocessing as mp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.model_selection as skms\n",
        "import sklearn.metrics as skmetrics\n",
        "from typing import Tuple\n",
        "if USING_COLAB:\n",
        "  from google.colab import drive\n",
        "  %pip install torchsummary"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Posteriormente, nos situamos en la carpeta donde se encuentra el dataset con `os.chdir`. Es importante mencionar que deberemos encontrarnos en la carpeta `practica2`, padre de este cuaderno y demás elementos de la entrega, para que se puedan usar las rutas definidas en este cuaderno sin necesidad de modificar nada. De usar Google Colab, deberá modificarse la ruta donde se encuentra el datset, de acuerdo con la organización de sus ficheros en este servicio. En cualquier caso, se asume que el conjunto de datos se encuentra descomprimido.\n",
        "\n",
        "Posteriormente, definimos dos constantes para distinguir entre el dataset reducido y el completo, generamos las rutas del sistema de ficheros a la carpeta con cada conjunto de datos y creamos la carpeta donde guardaremos los resultados (de no existir)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZHXYew96cu5",
        "outputId": "86b24282-ef1d-44f1-9564-e5eb2c02356a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results directory already exists, skipping creation.\n"
          ]
        }
      ],
      "source": [
        "if USING_COLAB:\n",
        "  # Mount Google Drive and move to dataset directory (it is already uploaded and unzipped)\n",
        "  drive.mount('/content/drive/')\n",
        "  os.chdir('/content/drive/MyDrive/Práctica 2 - Deep Learning para clasificación/dataset')\n",
        "else: # Local, move to folder of dataset asuming we are place on root directory for this practice (images already unzipped)\n",
        "  os.chdir(os.path.join(os.getcwd(), 'dataset'))\n",
        "  # chdir is done, so it execution fails, kernel needs to be restarted to avoid errors (or insert a os.chdir(../) to go back to root directory)))\n",
        "\n",
        "DATASET_20 = 'data x20'\n",
        "DATASET_200 = 'data x200'\n",
        "DATASET_20_PATH = os.path.join(os.getcwd(), DATASET_20) # No / at the end\n",
        "DATASET_200_PATH = os.path.join(os.getcwd(), DATASET_200)\n",
        "\n",
        "RANDOM_SEED = 42  # Keep consistency in random generators\n",
        "OUT_DIR = 'results'\n",
        "try:  # No overwrite already existing results\n",
        "  os.makedirs(OUT_DIR, exist_ok=False)\n",
        "except FileExistsError:\n",
        "  print('Results directory already exists, skipping creation.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que la ejecución de los diferentes entrenamientos de los modelos de clasificación podían tomar un tiempo considerable, se proporcionan una serie de variables que actuan como flags. Estas permiten \"activar\" y \"desactivar\" el entrenamiento de los diferentes modelos, así como su evaluación sobre el conjunto de test. También podemos controlar si queremos generar las gráficas con la evolución de las funciones de pérdida y la precisión.\n",
        "\n",
        "Dado que no se espera que se ejecute este cuaderno se entrega con todos los flags desactivados. De querer ejecutar algo, recuerde colocar los ficheros con los pesos pre-entrenados en el directorio `dataset/results`. Los ficheros con los pesos pre-entrenados se pueden descargar desde [este enlace a Zenodo](https://zenodo.org/record/7995859) (asegúrese de seleccionar la versión 2). Dado que no hemos utilizado previamente Zenodo, se proporciona [este enlace alternativo a MEGA](https://mega.nz/file/3joRRDzI#BLq3jt8LStjD6NeFaisGv8QKB-n4yIlLY4gdrTV4kHk), donde los ficheros permanecerán disponibles hasta la publicación de las actas de la asignatura por si hubiera algún tipo de problema. De no poder acceder de ninguna de las dos maneras, por favor contactar con la mayor celeridad posible a los correos especificados en el inicio de este documento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_BASELINE_MODELS = False\n",
        "TRAIN_PRETRAINED_MODELS = False\n",
        "TRAIN_EFFICIENTNET_MODELS = False\n",
        "TRAIN_MOBILENET_MODELS = False\n",
        "TRAIN_SMALL_MODELS = False\n",
        "TRAIN_BIG_MODELS = False\n",
        "TEST_SMALL_MODELS = False\n",
        "TEST_BIG_MODELS = False\n",
        "GENERATE_GRAPHS = False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, comprobamos si disponemos de GPU o si deberemos recurrir de la CPU del equipo. En la medida de lo posible, se recomienda encarecidamente hacer uso de GPU para poder realizar la ejecución en un tiempo asumible. De hacerse uso de Google Colab, recuerde que debe habilitar la GPU manualmente, y que la duración máxima de las ejecuciones dependerá de su plan de pago y del estado de los recursos de Google. En el plan gratuito, la ejecución finaliza a las 12 horas, tiempo que en principio debería ser más que suficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss9N8l64SaQd",
        "outputId": "e45ff2b6-84ef-4944-ea4f-b7c6a240f314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Using GPU: NVIDIA GeForce GTX 1080\n"
          ]
        }
      ],
      "source": [
        "# Setting up torch's device. (In Collab GPU has to be enable manually, and only for limited time)\n",
        "if torch.cuda.is_available():\n",
        "  DEVICE = torch.device(\"cuda\")\n",
        "  print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "  print(f'Using GPU: {torch.cuda.get_device_name(0)}')\n",
        "else:\n",
        "  DEVICE = torch.device(\"cpu\")\n",
        "  print('No GPU available, using the CPU instead.')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rF3j3qoPQqC8"
      },
      "source": [
        "## Análisis exploratorio\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como en cualquier proyecto de ciencia de datos, en primer lugar deberemos analizar el conjunto de datos con el que vamos a trabajar, de manera que tengamos un mejor entendimento del problema y podamos dilucidar que técnicas son las adecuadas para pre-procesar y manejar el conjunto de datos.\n",
        "\n",
        "Las tres funciones que se definen a continuación se encargan de obtener las rutas a todos los ficheros correspondientes a imágenes dentro de un determinado directorio, comprobar si una determinada imagen está corrupta y hacer la comprobación anterior para todo un directorio (eliminando aquellas que efectivamente estén corruptas). \n",
        "\n",
        "Para calcular si la imagen está corrupta se hace uso de la desviación estándar del valor de los píxeles; de ser esta cercana a 0 todos los píxeles tendrán un color igual o muy similar, síntoma habitual de que la imagen está corrupta y debe ser eliminada. \n",
        "\n",
        "En función del número de imágenes a procesar este cálculo puede tardar algunos minutos, por lo que se paraleliza el trabajo en tantos hilos de CPU como se pueda."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "USmIQ9mznfLH"
      },
      "outputs": [],
      "source": [
        "def get_filepaths(path_to_data: str, fileformat: str='.jpg') -> list:\n",
        "  \"\"\"\n",
        "  Returns paths to files of the specified format\n",
        "  \"\"\"\n",
        "  filepaths = list()\n",
        "  for root, _, filenames in os.walk(path_to_data):\n",
        "    for fn in filenames:\n",
        "      if fn.lower().endswith(fileformat):\n",
        "        filepaths.append(os.path.join(root,fn))\n",
        "\n",
        "  return filepaths\n",
        "\n",
        "def check_image_is_corruputed(path_to_img: str) -> Tuple[bool, str]:\n",
        "  \"\"\"\n",
        "  Return if the image is corrupted and the path to it\n",
        "  \"\"\"\n",
        "  std = np.std(mpimg.imread(path_to_img))\n",
        "  img_ok = not np.isclose(std, 0.0)\n",
        "  return img_ok, path_to_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2iCNYMt9sACk"
      },
      "outputs": [],
      "source": [
        "def check_and_delete_corrupt_images(path_to_dataset: str, dataset: str) -> None:\n",
        "  \"\"\"\n",
        "  Deletes the corrupt images given a dataset path and its name. \n",
        "  This takes quite some time.\n",
        "  \"\"\"\n",
        "  print(\"Going to use {} cores\".format(mp.cpu_count()))\n",
        "  \n",
        "  data_file_paths =  get_filepaths(path_to_dataset)\n",
        "  print(len(data_file_paths), \"images in\", dataset)\n",
        "\n",
        "  imgs_corrupted = list()\n",
        "  with mp.Pool(processes=mp.cpu_count()) as pool:\n",
        "    for img_ok, fn in pool.imap_unordered(check_image_is_corruputed, data_file_paths):\n",
        "      if not img_ok:\n",
        "          imgs_corrupted.append(fn)\n",
        "\n",
        "  print('Corrupted images:', len(imgs_corrupted))\n",
        "  for fn in imgs_corrupted:\n",
        "      os.remove(fn)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La comprobación y eliminación de imágenes corruptas sólo debe realizarse en la primera carga del conjunto de datos, por lo que se encuentra bajo el flag `EXECUTING_FOR_FIRST_TIME`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BdCfjkCjjUmK"
      },
      "outputs": [],
      "source": [
        "# Delete corrupt images in both datasets.\n",
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "  check_and_delete_corrupt_images(DATASET_20_PATH, DATASET_20)\n",
        "  check_and_delete_corrupt_images(DATASET_200_PATH, DATASET_200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación, visualicemos algunas imágenes de las aves que se se encuentran en el conjunto de datos. Por ejemplo, 5 imágenes aleatorias de gorriones.\n",
        "\n",
        "![Gorriones muestra](./img_for_docs/show_random_birds.png)\n",
        "\n",
        "Como podemos ver, las imágenes pueden tener diferentes dimensiones y orientación, lo cual deberemos tener en cuenta a la hora de procesar el conjunto de datos. Aunque lo trataremos más adelante, parace que los pájaros se suelen encontrar en la zona central de la imagen, y bien pueden estar mirando hacia la izquierda o derecha indistintamente. También cabe destacar que, para un ojo no experto, algunas aves de diferentes subespecies presentan grandes similitudes en su apariencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "    # Show some images of sparrows, using plt.show() as Visual Studio Code permits you to save images shown, easier file management.\n",
        "    random.seed(RANDOM_SEED)\n",
        "    img_sparrows = dict()\n",
        "    cls_sparrows_total = [k for k in os.listdir(DATASET_200_PATH) if 'sparrow' in k.lower()]\n",
        "    cls_sparrows = cls_sparrows_total[1::2][:5]\n",
        "    for dirname in cls_sparrows:\n",
        "        imgs = list()\n",
        "        for dp, _, fn in os.walk(os.path.join(DATASET_200_PATH, dirname)):\n",
        "            imgs.extend(fn)\n",
        "        img_sparrows[dirname] = imgs\n",
        "    n_cls = len(cls_sparrows)\n",
        "    f, ax = plt.subplots(1, n_cls, figsize=(14, 8))\n",
        "\n",
        "    for i in range(n_cls):\n",
        "        cls_name = cls_sparrows[random.randint(0, n_cls - 1)]\n",
        "        n_img = len(img_sparrows[cls_name])\n",
        "        img_name = img_sparrows[cls_name][random.randint(0, n_img - 1)]\n",
        "        path_img = os.path.join(os.path.join(DATASET_200_PATH, cls_name), img_name)\n",
        "        ax[i].imshow(mpimg.imread(path_img))\n",
        "        ax[i].set_title(cls_name.split('.')[-1].replace('_', ' '),  fontsize=12)\n",
        "\n",
        "        plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como mencionábamos anteriormente, las imágenes parecen ser de diferentes dimensiones. \n",
        "\n",
        "A continuación definimos una función para comprobar el tamaño de cada imagen en el conjunto de datos y calcular la media, de manera que tengamos información precisa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yFjX02zw4cpO"
      },
      "outputs": [],
      "source": [
        "def check_images_size_variation(path_to_dataset: str, dataset: str) -> None:\n",
        "    \"\"\"\n",
        "    Computes width and height for each image, results are represented via boxplot. \n",
        "    This may take some time\n",
        "    \"\"\"\n",
        "    ds = tv.datasets.ImageFolder(path_to_dataset)\n",
        "    shapes = [(img.height, img.width) for img, _ in ds]\n",
        "    heights, widths = [[h for h,_ in shapes], [w for _,w in shapes]]\n",
        "    print('Average sizes:', *map(np.median, zip(*shapes)))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    bp = ax.boxplot([heights, widths], patch_artist=True)\n",
        "    ax.set_xticklabels(['height', 'width'])\n",
        "    ax.set_xlabel('Image sizes for ' + dataset)\n",
        "    ax.set_ylabel('Pixels')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3IQi2oBKl9ow"
      },
      "outputs": [],
      "source": [
        "# Images are variable in size, let's see how much.\n",
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "  check_images_size_variation(DATASET_20_PATH, DATASET_20)\n",
        "  check_images_size_variation(DATASET_200_PATH, DATASET_200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La ejecución de la función anterior para ambos conjuntos de datos genera los diagramas de cajas y bigotes que se muestran a continuación.\n",
        "\n",
        "![Variabilidad tamaños dataset 20](./img_for_docs/image_variability_x20_dataset.png)\n",
        "![Variabilidad tamaños dataset 200](./img_for_docs/image_variability_x200_dataset.png)\n",
        "\n",
        "Como podemos comprobar, la mayoría de las imágenes del conjunto de datos reducidos tienen un alto de entre 330 y 430 píxeles, siendo la media 375 píxeles. El ancho por su parte suele ser superior a los 420 píxeles, y la media es cercana a los 500; aunque hay algunos outliers con un ancho menor a los 300 píxeles. Para el conjunto de datos completo la variabilidad en la altura de las imágenes es algo menor, aunque la media se sigue situando en 375 píxeles; el ancho por su parte suele ser de 500 píxeles, aunque se muestran bastantes outliers (círculos por debajo del primer cuartil)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que necesitamos que todas las imágenes tengan las mismas dimensiones, definimos una función que las redimensione (pad) a un tamaño dado, rellenando los píxeles adicionales con un determinado color (sólido negro por defecto). Esta función tiene en cuenta si se va a utilizar dentro de PyTorch o no, dado que la altura y anchura de la imagen se obtienen de forma diferente en función de si es un array de Numpy (para generar las imágenes que mencionaremos a continuación) o si es un tensor de PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Ctz6riqHm1Oi"
      },
      "outputs": [],
      "source": [
        "def pad(img, fill=0, size_max=500, used_for_pytorch=True):\n",
        "    \"\"\"\n",
        "    Pads images to the specified size (height x width). \n",
        "    Fills up the padded area with value(s) passed to the `fill` parameter. \n",
        "    \"\"\"\n",
        "    if used_for_pytorch:\n",
        "      pad_height = max(0, size_max - img.height)\n",
        "      pad_width = max(0, size_max - img.width)\n",
        "    else:\n",
        "      pad_height = max(0, size_max - img.shape[-2])\n",
        "      pad_width = max(0, size_max - img.shape[-1])\n",
        "\n",
        "    pad_top = pad_height // 2\n",
        "    pad_bottom = pad_height - pad_top\n",
        "    pad_left = pad_width // 2\n",
        "    pad_right = pad_width - pad_left\n",
        "    \n",
        "    return tv.transforms.functional.pad(img, (pad_left, pad_top, pad_right, pad_bottom), fill=fill)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuación, definimos una función para calcular el valor medio de cada píxel dadas las imágenes de un conjunto de datos. Con esta función pretendemos localizar donde se encuentran principalmente los pájaros (con colores significativamente distintos del fondo y entre sí), lo cual podría darnos alguna información de técnicas de procesamiento a utilizar, tipos concretos de redes neuronales que podamos usar o ajustes a sus parámetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SC8XxUp4p5sl"
      },
      "outputs": [],
      "source": [
        "def average_image_with_padding(path_to_dataset: str) -> None:\n",
        "    \"\"\"\n",
        "    Pad each image so it matches the maximum size of the dataset.\n",
        "    Calculates the mean of the image, so we can see where data is concentrated\n",
        "    \"\"\"\n",
        "    ds = tv.datasets.ImageFolder(path_to_dataset, transform=tv.transforms.ToTensor())\n",
        "    img_mean = np.zeros((3, 500, 500))\n",
        "    for img, _ in ds:\n",
        "        img = pad(img, used_for_pytorch=False)\n",
        "        img_mean += img.numpy()\n",
        "\n",
        "    img_mean = img_mean / len(ds)\n",
        "\n",
        "    # visualize the average image  \n",
        "    plt.imshow(np.moveaxis(img_mean, 0, 2))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ohp4MhLvq1p_"
      },
      "outputs": [],
      "source": [
        "# Gonna check where the birds are usually found in the images, as this may give us a clue for later processing.\n",
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "    average_image_with_padding(DATASET_20_PATH)\n",
        "    average_image_with_padding(DATASET_200_PATH)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las imágenes resultado de la ejecución del código anterior son las siguientes:\n",
        "\n",
        "![Main Info dataset x20](./img_for_docs/image_main_info_x20_dataset.png)\n",
        "![Main Info dataset x200](./img_for_docs/image_main_info_x200_dataset.png)\n",
        "\n",
        "Tanto para el conjunto pequeño (izquierda), como para el conjunto completo (derecha) la media presenta valores/colores distintos en la parte central de la imagen, por lo que podemos afirmar que los pájaros se suelen encontrar en la parte central de las imágenes. Esto parece concordar con lo que podíamos observar en un primer momento al ver algunas imágenes del conjunto de datos.\n",
        "\n",
        "El hecho de que los pájaros suelan encontrarse en el centro de las imágenes puede llevar al modelo a enfocarse principalmente en esa área, mientras ignora objetos relevantes ubicados en otras partes de la imagen. Intentaremos darle solución a este hecho algo más adelante con el aumento de datos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9yU5r4k7Q3oi"
      },
      "source": [
        "## Preparación de los datos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta sección prepararemos los datos y definiremos las estructuras necesarias para llevar a cabo los entrenamientos de los diferentes modelos de clasificación.\n",
        "\n",
        "Llegados a este momento cabe destacar que, aunque se han descargado las imágenes del fichero proporcionado por el profesorado de la asignatura y se sigue su estructura de directorios, se han descargado algunos ficheros adicionales de la web original de [Caltech Library](https://data.caltech.edu/records/65de6-vp158) donde se publicó el conjunto de datos. Concretamente, estos ficheros son `train_test_split` y `bounding_boxes`. \n",
        "\n",
        "El primero de ellos define si una imagen se emplea para el conjunto de entrenamiento (1) o para el conjunto de test (0), mientras que el segundo define las coordenadas de las bounding boxes que contienen al ave en cada imagen. Finalmente no hemos empleado las bounding boxes dado que no son estrictamente necesarias para un problema de clasificación, pero se mantiene el código asociado a su procesamiento dado que podrían ser útiles para modelos como YOLO, capaces de realizar tanto clasificación como segmentación, y que necesitan de esta información.\n",
        "\n",
        "Tanto estos ficheros, como el fichero `images`, han sido renombrados para mostrar al final el número de clases (y por tanto conjunto de datos al que hacen referencia, _x20 o _x200). Si se hace uso de los ficheros adjuntos en esta entrega, no necesita realizar ninguna gestión adicional; si por el contrario ha usado los descargados, por favor renómbrelos a `images_x200.txt`, `train_test_split_x200.txt` y `bounding_boxes_200.txt` (o modifique el código mostrado en las siguientes celdas).\n",
        "\n",
        "Dado que en principio sólo se nos proporcionan los ficheros para el conjunto de datos completo, necesitamos generar los equivalentes para el conjunto de datos pequeño. Esto no es estrictamente necesario, pues bastaría con coger las 1.155 (número de imágenes del conjunto pequeño) primeras líneas de los ficheros disponibles. Para facilitar el procesamiento y favorecer el \"separation of concerns\", buena práctica en informática, se generan unos nuevos ficheros con la función y código de las dos siguientes celdas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rK7etvaIM2q8"
      },
      "outputs": [],
      "source": [
        "def save_first_lines(input_file, output_file, num_lines):\n",
        "    \"\"\"\n",
        "    Given an input file, it stores the first num_lines in the given output_file\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r') as input_f:\n",
        "        lines = input_f.readlines()[:num_lines]\n",
        "\n",
        "    with open(output_file, 'w') as output_f:\n",
        "        output_f.writelines(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gA27nH0zyZxg"
      },
      "outputs": [],
      "source": [
        "# Original dataset provided us with data for 200 classes dataset, we are gonna generated the equivalent files for the data x20.\n",
        "# We could just use the first 1155 lines of the other files, but, for separation of concerns and easier processing, we are generating the new files\n",
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "    data_file_paths =  get_filepaths(DATASET_20_PATH)\n",
        "    save_first_lines(\n",
        "        os.path.join(os.path.join(os.getcwd(), 'data additional'), 'train_test_split_x200.txt'),\n",
        "        os.path.join(os.path.join(os.getcwd(), 'data additional'), 'train_test_split_x20.txt'),\n",
        "        len(data_file_paths))\n",
        "    \n",
        "    save_first_lines(\n",
        "        os.path.join(os.path.join(os.getcwd(), 'data additional'), 'images_x200.txt'),\n",
        "        os.path.join(os.path.join(os.getcwd(), 'data additional'), 'images_x20.txt'),\n",
        "        len(data_file_paths))\n",
        "\n",
        "    save_first_lines(\n",
        "        os.path.join(os.path.join(os.getcwd(), 'data additional'), 'bounding_boxes_x200.txt'),\n",
        "        os.path.join(os.path.join(os.getcwd(), 'data additional'), 'bounding_boxes_x20.txt'),\n",
        "        len(data_file_paths))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Llegados a este punto, disponemos de todos los elementos necesarios para crear el cargador de datos que utilizaremos posteriormente para entrenar los modelos de clasificación.\n",
        "\n",
        "Utilizaremos como cargador de datos la clase `DatasetBirds` que se define a continuación, y que hereda de la clase `ImageFolder` y consecuentemente de `DatasetFolder` de PyTorch.\n",
        "\n",
        "Necesitaremos obligatoriamente de la ruta a los datos, las transformaciones que se aplicarán a los datos (las definimos posteriormente), que cargador usar (por defecto), si estamos en entrenamiento o test y cómo chequear si un fichero es corrupto. Nótese que esta última función la marcamos como `None` dado que ya hemos elimiando los ficheros corruptos de antemano.\n",
        "\n",
        "Adicionalmente, se añaden dos parámetros opcionales y extra, que indican si usar bounding boxes (marcado a `False` dado que al final no las hemos usado) y el nombre del dataset a utilizar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9SM9P0BSBF_x"
      },
      "outputs": [],
      "source": [
        "class DatasetBirds(tv.datasets.ImageFolder):\n",
        "    \"\"\"\n",
        "    Wrapper for the CUB-200-2011 datasets. \n",
        "    Method DatasetBirds.__getitem__() returns tuple of image and its corresponding label.    \n",
        "    \"\"\"\n",
        "    def __init__(self, \n",
        "                 root, \n",
        "                 transform=None, \n",
        "                 target_transform=None, \n",
        "                 loader=tv.datasets.folder.default_loader,\n",
        "                 is_valid_file=None, \n",
        "                 train=True,\n",
        "                 bboxes=False,\n",
        "                 dataset=None):\n",
        "      \n",
        "      if os.path.exists(root): \n",
        "        img_root = root\n",
        "      else:\n",
        "        raise FileNotFoundError(f\"The path '{root}' does not exist.\")\n",
        "\n",
        "      if not img_root.endswith(dataset):\n",
        "        raise Exception(f\"Mismatch between the path to the dataset and the type of dataset specified.\")\n",
        "\n",
        "      super(DatasetBirds, self).__init__(\n",
        "          root=img_root, \n",
        "          transform=None, \n",
        "          target_transform=None,\n",
        "          loader=loader, \n",
        "          is_valid_file=is_valid_file\n",
        "      )\n",
        "\n",
        "      self.transform_ = transform\n",
        "      self.target_transform_ = target_transform\n",
        "      self.train = train\n",
        "\n",
        "      # Getting the number of classes from the dataset name\n",
        "      match = re.search(r'\\d+$', dataset)\n",
        "      if match:\n",
        "        num_classes = match.group(0)\n",
        "      else:\n",
        "        raise Exception(f\"Dataset type specified doesn't contain the number of classes at the end of the string. It should end in _x<number_classes>\")\n",
        "\n",
        "      train_test_split_filename = 'train_test_split_x' + num_classes + '.txt'\n",
        "      images_filename = 'images_x' + num_classes + '.txt'\n",
        "      bounding_boxes_filename = 'bounding_boxes_x' + num_classes + '.txt'\n",
        "\n",
        "      path_to_splits = os.path.join(os.path.join(os.getcwd(), 'data additional'), train_test_split_filename)\n",
        "      indices_to_use = list()\n",
        "      with open(path_to_splits, 'r') as in_file:\n",
        "          for line in in_file:\n",
        "              idx, use_train = line.strip('\\n').split(' ', 2)\n",
        "              if bool(int(use_train)) == self.train:\n",
        "                  indices_to_use.append(int(idx))\n",
        "      \n",
        "      path_to_index = os.path.join(os.path.join(os.getcwd(), 'data additional'), images_filename)\n",
        "      filenames_to_use = set()\n",
        "      with open(path_to_index, 'r') as in_file:\n",
        "          for line in in_file:\n",
        "              idx, fn = line.strip('\\n').split(' ', 2)\n",
        "              if int(idx) in indices_to_use:\n",
        "                  filenames_to_use.add(fn)\n",
        "\n",
        "      # Finally not using the bounding boxes, left in case we want to use them later for some model like YOLO.\n",
        "      if bboxes:\n",
        "          path_to_bboxes = os.path.join(os.path.join(os.getcwd(), 'data additional'), bounding_boxes_filename)\n",
        "          bounding_boxes = list()\n",
        "          with open(path_to_bboxes, 'r') as in_file:\n",
        "              for line in in_file:\n",
        "                  idx, x, y, w, h = map(lambda x: float(x), line.strip('\\n').split(' '))\n",
        "                  if int(idx) in indices_to_use:\n",
        "                      bounding_boxes.append((x, y, w, h))\n",
        "\n",
        "          self.bboxes = bounding_boxes\n",
        "      else:\n",
        "          self.bboxes = None\n",
        "\n",
        "      # / may be changed to \\ depending on the OS, we are asuming Linux.\n",
        "      img_paths_cut = {'/'.join(img_path.rsplit('/', 2)[-2:]): idx for idx, (img_path, lb) in enumerate(self.imgs)}\n",
        "      imgs_to_use = [self.imgs[img_paths_cut[fn]] for fn in filenames_to_use]\n",
        "\n",
        "      _, targets_to_use = list(zip(*imgs_to_use))\n",
        "\n",
        "      self.imgs = self.samples = imgs_to_use\n",
        "      self.targets = targets_to_use\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample, target = super(DatasetBirds, self).__getitem__(index)\n",
        "\n",
        "        if self.bboxes is not None:\n",
        "            width, _ = sample.width, sample.height\n",
        "            x, y, w, h = self.bboxes[index]\n",
        "            \n",
        "            # Some models,as YOLO, use different bbox formats. \n",
        "            scale_resize = 500 / width\n",
        "            scale_resize_crop = scale_resize * (375 / 500)\n",
        "\n",
        "            x_rel = scale_resize_crop * x / 375\n",
        "            y_rel = scale_resize_crop * y / 375\n",
        "            w_rel = scale_resize_crop * w / 375\n",
        "            h_rel = scale_resize_crop * h / 375\n",
        "\n",
        "            target = torch.tensor([target, x_rel, y_rel, w_rel, h_rel])\n",
        "\n",
        "        if self.transform_ is not None:\n",
        "            sample = self.transform_(sample)\n",
        "        if self.target_transform_ is not None:\n",
        "            target = self.target_transform_(target)\n",
        "\n",
        "        return sample, target\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esencialmente, el constructor:\n",
        "- Comprueba que el directorio donde están los datos concuerda con el nombre del conjunto de datos. No tendría sentido decir que vamos a usar el conjunto pequeño con los ficheros del grande, o viceversa.\n",
        "- Asigna las transformaciones que van a aplicarse sobre los datos para realizar un aumento de datos.\n",
        "- Marca si estamos en entrenamiento o test.\n",
        "- Lee de los correspondientes ficheros las rutas a las imágenes y cuáles se van a usar en entrenamiento y cuáles en test.\n",
        "- Genera para cada imagen una tupla con la ruta a dicha imagen y cual es la clase (derivada del nombre de la carpeta donde se ubica).\n",
        "- Asigna las imágenes a usar y los `targets`.\n",
        "\n",
        "También tenemos que sobreescribir el método indexador, u operador []. Dado un índice, debemos poder obtener la imagen (y clase asociada) en dicha posición, tras haber aplicado las transformaciones correspondientes.\n",
        "\n",
        "Nótese que aunque finalmente no se utilizan bounding boxes se mantiene la funcionalidad asociada implementada. Téngase también en cuenta que existen varios formatos para representar bounding boxes. En los ficheros de este conjunto de datos viene representadas por coordenadas y distancias absolutas, pero podrían seguir otros formatos."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V6el051LQ8dM"
      },
      "source": [
        "## Aumentando los datos"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este momento, definimos las transformaciones de las que haremos uso para aumentar los datos del conjunto de imágenes del que disponemos. \n",
        "\n",
        "Como se mencionaba anteriormente, necesitamos que todas las imágenes tengan las mismas dimensiones. Para conseguirlo, definimos una transformación de PyTorch (`max_padding`) haciendo uso de la función de padding que ya se comentó durante el análisis exploratorio. En esta ocasión, en lugar de rellenar con negro sólido se rellena con el color medio para el conjunto de datos de ImageNet (llevado al rango \\[0, 255\\]). Esta decisión se fundamenta en que en apartados posteriores partiremos del entrenamiento realizado haciendo uso de ciertos modelos sobre el conjunto de datos de ImageNet (compuesto por 1000 clases, entre las que hay algunas de animales)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pOgkqwheH5AQ"
      },
      "outputs": [],
      "source": [
        "# Images have different sizes, we are gonna use padding to the max dimensions in the dataset.\n",
        "fill = tuple(map(lambda x: int(round(x * 256)), (0.485, 0.456, 0.406))) # Mean pixels value of each pixel for ImageNet.\n",
        "max_padding = tv.transforms.Lambda(lambda x: pad(x, fill=fill))  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otras transformaciones que vamos a aplicar durante el entrenamiento de los modelos de clasificación es son el flip horizontal y vertical, el recorte aleatorio y los cambios de brillo, contraste y saturación. \n",
        "- El flip horizontal nos permitirá distinguir pájaros independienemente de la orientación a la que mire su pico (consultar imágenes del análisis exploratorio). \n",
        "- En este conjunto de datos la mayoría de los pájaros se encuentran posados en alguna superficie, por lo que su orientación es la habitual. Por tanto, podría pensarse que aplicar flip vertical es poco menos que inútil, sin embargo, de encontrarse en vuelo (o de tratar con pájaros como murciélagos) podríamos encontrarlo en otras orientaciones, por lo que nos anticipamos a estas situaciones.\n",
        "- La mayoría de los pájaros se encuentran en el centro de la imagen, pero podría no ser siempre así. Para conseguir que el modelo realice una mejor generalización realizamos un recorte aleatorio de la imagen.\n",
        "- Las fotografías podrían no encontrarse siempre perfectamente expuestas (contraste, brillo y saturación diferente a lo esperado, simplificando mucho), por lo que realizamos pequeñas variaciones de los mismos. No se aplican grandes variaciones bajo la suposición de que el color o tonalidad del pelaje del ave podría ser característico de una determinada especie. Recordamos una vez más que no somos expertos en la materia.\n",
        "\n",
        "Finalmente, convertimos a tensor y normalizamos con estadísticas propias de ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lv1M1HKUMk2r"
      },
      "outputs": [],
      "source": [
        "# Mayority of samples show the bird in the middle of image. \n",
        "# Test images will be center-cropped by 375x375 pixeles, and normalized by ImageNet's statistics \n",
        "transforms_train = tv.transforms.Compose([\n",
        "   max_padding,\n",
        "   tv.transforms.RandomOrder([\n",
        "       tv.transforms.RandomCrop((375, 375)),\n",
        "       tv.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "       tv.transforms.RandomHorizontalFlip(),\n",
        "       tv.transforms.RandomVerticalFlip()\n",
        "   ]),\n",
        "   tv.transforms.ToTensor(),\n",
        "   tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "transforms_eval = tv.transforms.Compose([\n",
        "   max_padding,\n",
        "   tv.transforms.CenterCrop((375, 375)),\n",
        "   tv.transforms.ToTensor(),\n",
        "   tv.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "]) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la evaluación nos limitaremos a realizar un recorte en el centro de la imagen (pues la mayoría de los pájaros se encuentran en esta posición) y a normalizar de acuerdo a valores propios de ImageNet."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creando los conjuntos de entrenamiento, validación y test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo siguiente que deberemos hacer es gener los conjuntos de entrenamiento, validación y test a partir de la clase de carga de datos definida anteriormente, a la que se le pasa como parámetros la ruta a los datos, el dataset a emplear y las tranformaciones que deseamos aplicar. Los splits los creamos haciendo uso de funciones de la librería Scikit-Learn. A partir de estos splits generamos los índices para entrenamiento y validación. Este proceso se replica para el conjunto de datos pequeño y para el conjunto completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2U6PSwvq0hoX"
      },
      "outputs": [],
      "source": [
        "# Create training, validation and test according to loaded splits.\n",
        "ds_x20_train = DatasetBirds(DATASET_20_PATH, transform=transforms_train, train=True, dataset=DATASET_20)     \n",
        "ds_x20_val = DatasetBirds(DATASET_20_PATH, transform=transforms_eval, train=True, dataset=DATASET_20)\n",
        "ds_x20_test = DatasetBirds(DATASET_20_PATH, transform=transforms_eval, train=False, dataset=DATASET_20)\n",
        "\n",
        "splits_x20 = skms.StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_SEED)\n",
        "idx_train_x20, idx_val_x20 = next(splits_x20.split(np.zeros(len(ds_x20_train)), ds_x20_train.targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "lChPATx6N9Np"
      },
      "outputs": [],
      "source": [
        "# Create training, validation and test according to loaded splits \n",
        "ds_x200_train = DatasetBirds(DATASET_200_PATH, transform=transforms_train, train=True, dataset=DATASET_200)\n",
        "ds_x200_val = DatasetBirds(DATASET_200_PATH, transform=transforms_eval, train=True, dataset=DATASET_200)\n",
        "ds_x200_test = DatasetBirds(DATASET_200_PATH, transform=transforms_eval, train=False, dataset=DATASET_200)\n",
        "\n",
        "splits_x200 = skms.StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_SEED)\n",
        "idx_train_x200, idx_val_x200 = next(splits_x200.split(np.zeros(len(ds_x200_train)), ds_x200_train.targets))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También deberemos definir los hiperparámetros con los que vamos a realizar el entrenamiento de la red. Estos deben definirse en función de si usamos Google Colab o no, y en función del hardware de nuestro equipo.\n",
        "\n",
        "Si usamos el plan gratuito de Google Colab disponemos de 2 hilos de CPU y una tarjeta gráfica NVIDIA TESLA T4, con 16 GB de memoria VRAM. Esto nos permitiría especificar un tamaño de lote de 32 imágenes; necesitando 11 GB de la memoria de la GPU para realizar el entrenamiento. No se ha probado demasiado dado que finalmente las ejecuciones se han realizado en la máquina local, pero esta GPU podría llegar a albergar lotes de tamaño 40 o incluso 48.\n",
        "\n",
        "De usar nuestra máquina local, debemos definir los parámetros en función a las características concretas de nuestro hardware. En lo relativo al número de hilos de CPU se ha definido que se usen como mínimo 2, y como máximo tantos como disponga el procesador-2 (para no saturarlo por si tuvieramos que hacer otra cosa mientras). \n",
        "\n",
        "Dada nuestra tarjeta gráfica, NVIDIA GTX 1080, con 8 GB de VRAM, podemos llegar a emplear lotes de tamaño 24 o inferior. De usar tamaño de lote 24 y ResNet-50 se necesitan unas 7.4 GB de VRAM, las cuales normalmente están disponibles, pero podría no ser el caso de usar varias pantallas o emplear otros programas durante el entrenamiento (lo cual no sería recomendable). Con tamaño de lote 16 se usan unas 5.5 GB de RAM, y no se tiene problema alguno durante la ejecución del entrenamiento del modelo ResNet-50. De tener tarjetas gráficas de menor capacidad deberá rebajarse el tamaño de lote, o usar Google Colab de ser posible. El modelo MobileNetV3 gasta menos memoria y no se tiene problema para ejecutarlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "JW4L2JcxPskM"
      },
      "outputs": [],
      "source": [
        "# Set hyper-parameters.\n",
        "# Not recommended to use more workers than CPU threads\n",
        "if USING_COLAB: \n",
        "  params = {'batch_size': 32, 'num_workers': 2} # Colab has CPU with 2 threads and TESLA T4.\n",
        "else:\n",
        "  params = {'batch_size': 16, 'num_workers': max(mp.cpu_count()-2, 2)}  \n",
        "  # Local machine has 12 CPU Cores and 1 GPU (GTX 1080) with 8 GB of VRAM. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, definimos las estructuras encargadas de cargar los datos a partir de los propios conjuntos de datos que acabamos de definir, el sampler y los hiperparámetros que acabamos de comentar.  Como sampler se usa el `SubsetRandomSampler` para elegir un subconjunto de los datos de forma aleatoria, el cual podría ser ciertamente numeroso dado el tamaño del conjunto de datos tras haber aplicado las transformaciones para el aumento de datos.\n",
        "\n",
        "Este proceso se realiza tanto para el conjunto pequeño como para el conjunto completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dr7Q-qAePxLV"
      },
      "outputs": [],
      "source": [
        "# Instantiate data loaders\n",
        "train_loader_x20 = torch.utils.data.DataLoader(\n",
        "   dataset=ds_x20_train,\n",
        "   sampler=torch.utils.data.SubsetRandomSampler(idx_train_x20),\n",
        "   **params\n",
        ")\n",
        "val_loader_x20 = torch.utils.data.DataLoader(\n",
        "   dataset=ds_x20_val,\n",
        "   sampler=torch.utils.data.SubsetRandomSampler(idx_val_x20),\n",
        "   **params\n",
        ")\n",
        "test_loader_x20 = torch.utils.data.DataLoader(dataset=ds_x20_test, **params)\n",
        "\n",
        "train_loader_x200 = torch.utils.data.DataLoader(\n",
        "   dataset=ds_x200_train,\n",
        "   sampler=torch.utils.data.SubsetRandomSampler(idx_train_x200),\n",
        "   **params\n",
        ")\n",
        "val_loader_x200 = torch.utils.data.DataLoader(\n",
        "   dataset=ds_x200_val,\n",
        "   sampler=torch.utils.data.SubsetRandomSampler(idx_val_x200),\n",
        "   **params\n",
        ")\n",
        "test_loader_x200 = torch.utils.data.DataLoader(dataset=ds_x200_test, **params)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5jxc3Rg0RC4L"
      },
      "source": [
        "## Clasificador ResNet-50 \"desde cero\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras haber definido las particiones para entrenamiento, validación y test, debemos definir un modelo de aprendizaje profundo que las use.\n",
        "\n",
        "Podríamos desarrollar una arquitectura propia desde cero, pero en su lugar, vamos a utilizar alguna arquitectura ya implementada en PyTorch. Este paquete software proporciona una larga lista de arquitecturas, como AlexNet, EfficientNet, MobileNet, VGG, entre otras. \n",
        "\n",
        "Tras haber consultado las especificaciones y algunas estadísticas de cada una de ellas en la documentación de PyTorch, elegimos utilizar la familia de arquitecturas ResNet, en tanto que proporcionan una buena precisión sobre el conjunto de datos ImageNet a cambio de una potencia computacional menor que otras alternativas consultadas, y en principio asumible por el hardware del que disponemos.\n",
        "\n",
        "En PyTorch disponemos de 5 implementaciones de ResNet, con 18, 34, 50, 101 y 152 capas respectivamente. Elegimos hacer uso de la versión con 50 capas en tanto que parece ofrecer la mejor relación entre precisión y coste computacional. La versión con 34 capas ofrece cerca de un 7% menos de precisión con un coste computacional sólo 11% menor, mientras que las versiones más complejas ofrecen 1.02% y 1.4% más de precisión con un coste computacional casi del doble y triple, respectivamente.\n",
        "\n",
        "La arquitectura ResNet fue propuesta por Kaiming He, Xiangyu Zhang, Shaoqing Ren y Jian Sun (Microsoft Research) en 2015 en un intento de solucionar el problema del \"desvanecimiento del gradiente\", y aunque el objetivo de esta práctica no es el análisis teórico de la arquitectura sí que destacaremos su principal innovación. \n",
        "\n",
        "Esta arquitectura incorpora las llamadas \"capas residuales\", de ahí el nombre. En lugar de intentar aprender directamente la representación deseada en cada capa, las conexiones residuales permiten a la red aprender las diferencias entre la salida deseada y la salida actual de la capa. Esto se logra añadiendo conexiones directas, llamadas \"saltos\", que saltan una o más capas y habilitan que el gradiente fluya por la red evitando su desvanecimiento. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kb56kn_JRHTL"
      },
      "outputs": [],
      "source": [
        "model_x20 = tv.models.resnet50(num_classes=20).to(DEVICE)\n",
        "model_x200 = tv.models.resnet50(num_classes=200).to(DEVICE)\n",
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "    print(model_x20) # Print default version (groupped with all info)\n",
        "    torchsummary.summary(model_x20, input_size=(3,375,375)) # Print the summary formatted, it is the same for both models."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras haber instanciado el modelo, podríamos consultar las capas concretas por las que está formado. Dada la gran extensión de la salida no se mostrará en este cuaderno, pero sí que destacaremos que tiene 23.549.012 parámetros entreanables y que ocupa 907.08 MB en memoria. \n",
        "\n",
        "También cabe mencionar que la arquitectura ResNet implementada en PyTorch es distinta de la propuesta originalmente por los autores en 2015. Si acudimos a la documentación de PyTorch, esta nos indica que \"the bottleneck of TorchVision places the stride for downsampling to the second 3x3 convolution while the original paper places it to the first 1x1 convolution. This variant improves the accuracy and is known as ResNet V1.5\".\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1iK4pAc-jFAU"
      },
      "source": [
        "### Entrenamiento y evaluación"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de comenzar con el entrenamiento de nuestro modelo, definiremos tres funciones que necesitaremos más tarde.\n",
        "\n",
        "La primera de ellas simplemente genera una cadena de texto en función del tipo de modelo que estemos entrenando (pues tendremos más en esta práctica a parte del descrito en esta sección), de cara al nombre con el que guardaremos los ficheros con los pesos resultantes del entrenamiento. Las otras dos funciones las emplearemos para generar gráficos a partir de los valores de pérdida y precisión que obtengamos tras entrenar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gN_CJNx10VSF"
      },
      "outputs": [],
      "source": [
        "def get_model_desc(num_classes=200, pretrained=False, model=None):\n",
        "    \"\"\"\n",
        "    Generates description string. Used for generated the name of the model when saving it to disk. \n",
        "    \"\"\"\n",
        "    desc = list()\n",
        "    if model == 'resnet50':\n",
        "        desc.append('ResNet50')\n",
        "    elif model == 'mobilenet':\n",
        "        desc.append('MobileNet')\n",
        "    elif model == 'efficientnet':\n",
        "        desc.append('EfficientNet')\n",
        "\n",
        "    if pretrained:\n",
        "        desc.append('Transfer')\n",
        "    else:\n",
        "        desc.append('Baseline')\n",
        "\n",
        "    return '_'.join(desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def graph_loss_function(train_loss, val_loss):\n",
        "    \"\"\"\n",
        "    Graphs the evolution of the loss function. \n",
        "    Requires both the loss of the training and validation set.\n",
        "    \"\"\"\n",
        "    x = range(1,len(train_loss)+1)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(x, train_loss, label='Entrenamiento')\n",
        "    plt.plot(x, val_loss, label='Validación')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Pérdida')\n",
        "    plt.title('Evolución de la función de pérdida')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def graph_accuracy_function(train_acc, val_acc, max_value = 1.01):\n",
        "    \"\"\"\n",
        "    Graphs the evolution of the accuracy function.\n",
        "    Requires both the accuracy of the training and validation set.\n",
        "    As optional parameter, the maximum value of the y axis can be specified. It has to be a float a bit higher than the strict limit.\n",
        "    \"\"\"\n",
        "    x = range(1,len(train_acc)+1)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(x, train_acc, label='Entrenamiento')\n",
        "    plt.plot(x, val_acc, label='Validación')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Precisión')\n",
        "    plt.yticks(np.arange(0, max_value, 0.05))\n",
        "    plt.title('Evolución de la precisión')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder entrenar el modelo necesitaremos definir un optimizador y un \"scheduler\" (actualizador de la tasa de aprendizaje) .\n",
        "\n",
        "El optimizador se encargará de actualizar los parámetros del modelo en función del gradiente calculado durante la retropropagación; mientras que el actualizador de tasa de aprendizaje controlará cómo se ajusta la tasa de aprendizaje a medida que avanza el entrenamiento (reduciendolo exponencialmente por un factor `gamma` tras cada época). \n",
        "\n",
        "Siguiendo las recomendaciones y experimentos llevados a cabo en la asignatura \"Inteligencia Computacional\" de estos mismos estudios de máster, decidimos hacer uso del optimizador ADAM al proporcionar este un mejor desempeño que alternativas como SGD o RMSprop. Los valores para las tasas del `learning rate` y `gamma` se toman de la bibliografía consultada, pero concuerdan con lo que vimos en la asignatura anteriormente mencionada durante el pasado cuatrimestre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3Cj1FeLIjEZO"
      },
      "outputs": [],
      "source": [
        "optimizer_x20 = torch.optim.Adam(model_x20.parameters(), lr=1e-3)\n",
        "scheduler_x20 = torch.optim.lr_scheduler.ExponentialLR(optimizer_x20, gamma=0.95)\n",
        "\n",
        "optimizer_x200 = torch.optim.Adam(model_x200.parameters(), lr=1e-3)\n",
        "scheduler_x200 = torch.optim.lr_scheduler.ExponentialLR(optimizer_x200, gamma=0.95)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para evitar la repetición de código, ya que entrenaremos varios modelos durante esta práctica, definimos dos funciones encargadas de realizar el entrenamiento en sí mismo y posteriormente el test del modelo generado.\n",
        "\n",
        "La función de entrenamiento requerirá del modelo, optimizador y \"scheduler\" a utilizar, así como los objetos encargados de cargar los datos de los conjuntos de entrenamiento y validación. También se necesita del tipo de conjunto de datos a utilizar y el flag `pretrained` para generar el nombre del fichero en el que se guardarán los pesos. Como no podía ser de otra manera, también necesitaremos especificar el número de épocas durante las cuales deseamos entrenar. \n",
        "\n",
        "Esta función nos devolverá la ruta al fichero en el que se han guardado los pesos, los cuales serán los mejores obtenidos durante el entrenamiento, y cuatro listas con los valores de la función de pérdida y precisión para el conjunto de entrenamiento y validación en cada época. Los valores de estas cuatro listas las utilizaremos para generar algunas gráficas.\n",
        "\n",
        "Por su parte, la función de test solamente requiere del modelo a testear, la ruta desde la cual cargar los pesos que se deseen usar y el objeto encargado de cargar las imágenes que van a ser usadas para el test. Esta función simplemente devolverá la precisión obtenida por el modelo para el conjunto de test especificado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WD1oYai5lqPb"
      },
      "outputs": [],
      "source": [
        "def train_validate_model(model, scheduler, optimizer,\n",
        "                                train_loader, val_loader, dataset: str, \n",
        "                                num_epochs: int,  modelname: str, pretrained: bool = False) -> Tuple[str, list, list, list, list]:\n",
        "  \"\"\"\n",
        "  Function to train and validate a model given a type of dataset\n",
        "  It save the weights in a .pt file ans returns the path to it\n",
        "  Returns:\n",
        "    - best_snapshot_path: path to the best model snapshot\n",
        "    - all_train_loss: list of all training losses mean values\n",
        "    - all_train_acc: list of all training accuracies mean values\n",
        "    - all_val_loss: list of all validation losses mean values\n",
        "    - all_val_acc: list of all validation accuracies mean values\n",
        "  \"\"\"\n",
        "\n",
        "  # Check consistency of dataset type\n",
        "  match = re.search(r'\\d+$', dataset)\n",
        "  if match:\n",
        "    num_classes = match.group(0)\n",
        "  else:\n",
        "    raise Exception(f\"Dataset type specified doesn't contain the number of classes at the end of the string. It should end in _x<number_classes>\")\n",
        "\n",
        "  model_desc = get_model_desc(int(num_classes), pretrained, modelname)\n",
        "  best_snapshot_path: str = \"\"\n",
        "  best_val_acc = 0.0\n",
        "  all_train_loss = list() # Mean values\n",
        "  all_train_acc = list()\n",
        "  all_val_loss = list()   # Mean values\n",
        "  all_val_acc = list()  \n",
        "\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = list()\n",
        "    train_acc = list()\n",
        "    for batch in train_loader:\n",
        "        x, y = batch\n",
        "        \n",
        "        x = x.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # predict bird species\n",
        "        y_pred = model(x)\n",
        "\n",
        "        # calculate the loss\n",
        "        loss = F.cross_entropy(y_pred, y)\n",
        "        # calculate the accuracy\n",
        "        acc = skmetrics.accuracy_score([val.item() for val in y], [val.item() for val in y_pred.argmax(dim=-1)])\n",
        "        \n",
        "        # backprop & update weights \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(acc)\n",
        "\n",
        "    all_train_loss.append(np.mean(train_loss))\n",
        "    all_train_acc.append(np.mean(train_acc))\n",
        "\n",
        "    # validate the model\n",
        "    model.eval()\n",
        "    val_loss = list()\n",
        "    val_acc = list()\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            x, y = batch\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            # calculate the loss\n",
        "            loss = F.cross_entropy(y_pred, y)\n",
        "            # calculate the accuracy\n",
        "            acc = skmetrics.accuracy_score([val.item() for val in y], [val.item() for val in y_pred.argmax(dim=-1)])\n",
        "\n",
        "            val_loss.append(loss.item())\n",
        "            val_acc.append(acc)\n",
        "\n",
        "        all_val_acc.append(np.mean(val_acc))\n",
        "        all_val_loss.append(np.mean(val_loss))\n",
        "            \n",
        "        # save the best model snapshot\n",
        "        current_val_acc = all_val_acc[-1]\n",
        "        if current_val_acc > best_val_acc:\n",
        "            print(\"New best accuracy {:.5f} > {:.5f} at epoch {}\".format(current_val_acc, best_val_acc, epoch+1))\n",
        "            if best_snapshot_path != \"\":\n",
        "                os.remove(best_snapshot_path)\n",
        "\n",
        "            best_val_acc = current_val_acc\n",
        "            best_snapshot_path = os.path.join(OUT_DIR, f'model_{model_desc}_x{num_classes}_ep={epoch+1}_acc={best_val_acc}.pt')\n",
        "\n",
        "            torch.save(model.state_dict(), best_snapshot_path)\n",
        "\n",
        "    # Adjust the learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "    if (epoch == 0) or ((epoch + 1) % 3 == 0 or (epoch + 1) == num_epochs):\n",
        "        print('Epoch {} |> Train. loss: {:.4f} | Val. loss: {:.4f}'.format(\n",
        "            epoch + 1, np.mean(train_loss), np.mean(val_loss))\n",
        "        )\n",
        "        print('Epoch {} |> Train. acc.: {:.4f} | Val. acc.: {:.4f}'.format(\n",
        "            epoch + 1, np.mean(train_acc), np.mean(val_acc))\n",
        "        )\n",
        "\n",
        "  return best_snapshot_path, all_train_loss, all_train_acc, all_val_loss, all_val_acc\n",
        "\n",
        "def test_model(model, test_loader, best_snapshot_path):\n",
        "  \"\"\"\n",
        "  Function to test a model given the test loader and the path to a snapshot of the model (theorically the best one)\n",
        "  Returns the accuracy\n",
        "  \"\"\"\n",
        "\n",
        "  model.load_state_dict(torch.load(best_snapshot_path, map_location=DEVICE))\n",
        "        \n",
        "  true = list()\n",
        "  pred = list()\n",
        "  with torch.no_grad():\n",
        "      for batch in test_loader:\n",
        "          x, y = batch\n",
        "          x = x.to(DEVICE)\n",
        "          y = y.to(DEVICE)\n",
        "          y_pred = model(x)\n",
        "\n",
        "          true.extend([val.item() for val in y])\n",
        "          pred.extend([val.item() for val in y_pred.argmax(dim=-1)])\n",
        "\n",
        "  \n",
        "  test_accuracy = skmetrics.accuracy_score(true, pred)\n",
        "  print('\\nTest accuracy: {:.3f}'.format(test_accuracy))\n",
        "  return test_accuracy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que los entrenamientos pueden tomar un tiempo considerable en función de la arquitectura empleada, conjunto de datos y número de épocas; antes de cada entrenamiento definiremos la ruta a los mejores resultados que hemos obtenido. Téngase la precaución de descargar los ficheros con los pesos (cuyos enlaces se encontrarán al final de este documento) y colocarlos en la carpeta `dataset/results`. No se comprueba si la ruta es válida, por lo que si se ejecutan sólamnete los test sin entrenar y sin disponer de los ficheros se producirá un error.\n",
        "\n",
        "Téngase en cuenta que de ejecutar el entrenamiento de un modelo se tomará la ruta al fichero con los pesos que mejores resultados han proporcionado durante dicho entrenamiento, ignorandose aquellos ficheros de los que pudieramos disponer en la carpeta y que eventualmente podrían ser mejores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "8u_3gT51perW"
      },
      "outputs": [],
      "source": [
        "BEST_BASELINE_RESNET_X20_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_ResNet50_Baseline_x20_ep=47_acc=0.5625.pt')\n",
        "BEST_BASELINE_RESNET_X200_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_ResNet50_Baseline_x200_ep=30_acc=0.23355263157894737.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, ejecutamos el entrenamiento del modelo para el conjunto de datos pequeño y graficamos la evolución de la función de pérdida y precisión durante el mismo.\n",
        "\n",
        "Los resultados que se muestran en este reporte son los mejores obtenidos, aunque se discutirán resultados obtenidos con otras configuraciones distitnas a la mencionada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59c9lZI4KUnu",
        "outputId": "2abe4a40-9ba5-49bb-86ab-1c7d15b5c403"
      },
      "outputs": [],
      "source": [
        "train_loss_baseline_resnet_x20, train_acc_baseline_resnet_x20, val_loss_baseline_resnet_x20, val_acc_baseline_resnet_x20 = list(), list(), list(), list()\n",
        "\n",
        "if TRAIN_BASELINE_MODELS and TRAIN_SMALL_MODELS:\n",
        "  BASELINE_RESNET_X20_RESULTS = train_validate_model(model_x20, scheduler_x20, optimizer_x20, train_loader_x20, val_loader_x20, DATASET_20, 70, 'resnet50')\n",
        "  BEST_BASELINE_RESNET_X20_PATH = BASELINE_RESNET_X20_RESULTS[0]\n",
        "  train_loss_baseline_resnet_x20, train_acc_baseline_resnet_x20, val_loss_baseline_resnet_x20, val_acc_baseline_resnet_x20 = BASELINE_RESNET_X20_RESULTS[1:]\n",
        "\n",
        "if TEST_SMALL_MODELS:\n",
        "  baseline_x20_accuracy = test_model(model_x20, test_loader_x20, BEST_BASELINE_RESNET_X20_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_BASELINE_MODELS and TRAIN_SMALL_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_baseline_resnet_x20, val_loss_baseline_resnet_x20)\n",
        "    graph_accuracy_function(train_acc_baseline_resnet_x20, val_acc_baseline_resnet_x20, max_value=0.81)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los mejores resultados han sido obtenidos empleando un `batch_size=16` y las técnicas de aumento de datos mencionadas anteriormente. En estas condiciones, conseguimos una precisión sobre el conjunto de test de 0.47 tras 70 épocas de entrenamiento y 16 minutos y medio de ejecución. Las gráficas con la evolución de la función de pérdida y precisión para entrenamiento y validación se muestra a continuación.\n",
        "\n",
        "![Baseline x20 loss function](./img_for_docs/baseline_loss_resnet_x20.png)\n",
        "![Baseline x20 acc function](./img_for_docs/baseline_acc_resnet_x20.png)\n",
        "\n",
        "A partir de la gráfica de la función de pérdida podemos intuir que se produce un \"overfitting\" o sobre-ajuste del modelo al conjunto de entrenamiento, pues el valor de dicha función no para de descender para el conjunto de entrenamiento, pero parece estabilizarse dentro de un rango para el conjunto de validación. Algo similar se puede observar en la gráfica que ilustra la evolución de la precisión. \n",
        "\n",
        "Los resultados obtenidos son ciertamente pobres, pues el clasificador falla en más ocasiones de las que acierta. \n",
        "\n",
        "Se ha probado también a ejecutar entrenamientos con `batch_size=16` prescindiendo del aumento de datos que conseguimos al variar brillo, contraste y saturación; en cuyo caso se ha obtenido una precisión máxima de 0.45. También se han ejecutado experimentos con `batch_size=24`, con y sin variación de brillo, contraste y saturación. En esta última casuística se han conseguido resultados peores, con precisiones que oscilaban entre 0.32 y 0.38, y no se consiguió una reducción en el tiempo de entrenamiento."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vZq9Q5rNxvnq"
      },
      "source": [
        "Los resultados obtenidos para el conjunto de datos pequeños no son demasiado esperanzadores, pero aun así decidimos ejecutar el entrenamiento para el conjunto de datos completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-qv-qRL6KXVN"
      },
      "outputs": [],
      "source": [
        "train_loss_baseline_resnet_x200, train_acc_baseline_resnet_x200, val_loss_baseline_resnet_x200, val_acc_baseline_resnet_x200 = list(), list(), list(), list()\n",
        "\n",
        "if TRAIN_BASELINE_MODELS and TRAIN_BIG_MODELS:\n",
        "  BASELINE_RESNET_X200_RESULTS = train_validate_model(model_x200, scheduler_x200, optimizer_x200, train_loader_x200, val_loader_x200, DATASET_200, 30, 'resnet50')\n",
        "  BEST_BASELINE_RESNET_X200_PATH = BASELINE_RESNET_X200_RESULTS[0]\n",
        "  train_loss_baseline_resnet_x200, train_acc_baseline_resnet_x200, val_loss_baseline_resnet_x200, val_acc_baseline_resnet_x200 = BASELINE_RESNET_X200_RESULTS[1:]\n",
        "\n",
        "if TEST_BIG_MODELS: \n",
        "  baseline_x200_accuracy = test_model(model_x200, test_loader_x200, BEST_BASELINE_RESNET_X200_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_BASELINE_MODELS and TRAIN_BIG_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_baseline_resnet_x200, val_loss_baseline_resnet_x200)\n",
        "    graph_accuracy_function(train_acc_baseline_resnet_x200, val_acc_baseline_resnet_x200, max_value=0.36)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados obtenidos son ciertamente desastrosos, pues apenas conseguimos una precisión máxima de 0.2335 para `batch_size=16` y 30 épocas tras un tiempo de ejecución de 69 minutos. Ejecuciones con tamaño de lote de 16, con o sin variación en saturación, brillo e intensidad proporcionan resultados similares (oscilan entre 0.20 y 0.225). Dudamos que con un mayor número de épocas consiguieramos aumentar mucho la precisión, por lo que no se ha probado con más de 30 épocas dado que el tiempo de ejecución comienza a ser \"poco asumible\".\n",
        "\n",
        "Las gráficas con la evolución de la función de pérdida y precisión para entrenamiento y validación se muestran a continuación, aunque no realicemos una discusión sobre las mismas.\n",
        "\n",
        "![Baseline x200 loss function](./img_for_docs/baseline_loss_resnet_x200.png)\n",
        "![Baseline x200 acc function](./img_for_docs/baseline_acc_resnet_x200.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados obtenidos en este apartado eran esperables, pues apenas disponemos de imágenes de cada clase como para entrenar a un clasificador \"desde cero\". \n",
        "\n",
        "En el siguiente apartado realizaremos un proceso de transferencia de aprendizaje (transfer learning o fine tuning) con el objetivo de conseguir un modelo útil que proporcione mejores decisiones."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dxQ9Fz940b-8"
      },
      "source": [
        "## Clasificador ResNet-50 a partir de pesos pre-entrenados"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como discutíamos anteriormente, si entrenamos un modelo desde cero con un número reducido de imágenes (incluso si empleamos aumento de datos) para cada clase estamos avocados a obtener un modelo que va a cometer un gran número de fallos en sus predicciones.\n",
        "\n",
        "Una posible solución a la problemática anterior es partir de los pesos obtenidos al entrenar un modelo con una gran cantidad de imágenes y realizar un proceso de ajuste para que estos se adecuen a nuestro conjunto de datos. Este enfoque nos permite obtener un buen rendimiento y una buena generalización del modelo utilizando un menor número de datos, especialmente si el conjunto de datos con el que se entrenó el modelo original está relacionado con el conjunto que vamos a emplear para realizar el ajuste. Además, este proceso de transferencia de aprendizaje permite reducir significativamente los tiempos de entrenamiento.\n",
        "\n",
        "Así pues, vamos a crear una nueva instancia del modelo ResNet-50 indicando que se utilicen pesos pre-entreandos. PyTorch nos proporciona la posibilidad de utilizar los pesos obtenidos al entrenar esta arquitectura de red sobre el conjunto de imágenes ImageNet en dos variantes. Haciendo uso de `ResNet50_Weights.DEFAULT` nos aseguramos utilizar aquellos pesos que proporcionan un mejor desempeño.\n",
        "\n",
        "ImageNet es un conjunto de datos con imágenes de 1000 clases muy variadas, entre las que se encuentran algunos tipos de animales, y lo que para nosotros es muy interesante y beneficioso, aves. Por tanto, esperamos obtener un rendimiento significativamente mejor que el mostrado en el apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xGH_vu7L0Zph"
      },
      "outputs": [],
      "source": [
        "# Using pretrained weights on ImageNet. Default weights may vary on Torch version, currently they are IMAGENET1K_V2\n",
        "model_pretrained = tv.models.resnet50(weights=tv.models.ResNet50_Weights.DEFAULT).to(DEVICE)\n",
        "new_optimizer = torch.optim.Adam(model_pretrained.parameters(), lr=1e-4)\n",
        "new_scheduler = torch.optim.lr_scheduler.ExponentialLR(new_optimizer, gamma=0.95)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UvW6s01f1vZR"
      },
      "outputs": [],
      "source": [
        "BEST_PRETRAINED_RESNET_X20_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_ResNet50_Transfer_x20_ep=3_acc=0.9375.pt')  \n",
        "BEST_PRETRAINED_RESNET_X200_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_ResNet50_Transfer_x200_ep=26_acc=0.8157894736842105.pt') "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras haber definido el nuevo modelo, y re-definir el optimizador y \"scheduler\" en consecuencia, procedemos a realizar el entrenamiento del modelo sobre el conjunto de datos pequeño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5drqE-_DKamU"
      },
      "outputs": [],
      "source": [
        "train_loss_pretrained_resnet_x20, train_acc_pretrained_resnet_x20, val_loss_pretrained_resnet_x20, val_acc_pretrained_resnet_x20 = list(), list(), list(), list()\n",
        "\n",
        "if TRAIN_PRETRAINED_MODELS and TRAIN_SMALL_MODELS:\n",
        "  PRETRAINED_RESNET_X20_RESULTS = train_validate_model(model_pretrained, new_scheduler, new_optimizer, train_loader_x20, val_loader_x20, DATASET_20, 30, 'resnet50', True)\n",
        "  BEST_PRETRAINED_RESNET_X20_PATH = PRETRAINED_RESNET_X20_RESULTS[0]\n",
        "  train_loss_pretrained_resnet_x20, train_acc_pretrained_resnet_x20, val_loss_pretrained_resnet_x20, val_acc_pretrained_resnet_x20 = PRETRAINED_RESNET_X20_RESULTS[1:]\n",
        "if TEST_SMALL_MODELS:\n",
        "  pretrained_x20_accuracy = test_model(model_pretrained, test_loader_x20, BEST_PRETRAINED_RESNET_X20_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_PRETRAINED_MODELS and TRAIN_SMALL_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_pretrained_resnet_x20, val_loss_pretrained_resnet_x20)\n",
        "    graph_accuracy_function(train_acc_pretrained_resnet_x20, val_acc_pretrained_resnet_x20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta ocasión, los resultados son muchos mejores que los obtenidos en el apartado anterior, ya que conseguimos una precisión sobre el conjunto de test del 94%. Además, nótese que se ha necesitado un número de épocas bastante menor para conseguir dicho resultado, lo que ha permitido reducir el tiempo de entrenamiento a apenas 7 minutos.\n",
        "\n",
        "Los mejores resultados se han obtenido haciendo uso de `batch_size=16` y el proceso de aumento de datos completo. Experimentos con este tamaño de lote sin variación en brillo, tono o saturación, y experimentos con `batch_size=24` aportan resultados muy similares; oscilando la precisión sobre el conjunto de test entre 0.91 y 0.932. \n",
        "\n",
        "Si observamos las gráficas con la evolución de la función de pérdida y la precisión, podemos ver como el modelo se ajusta perfectamente al conjunto de entrenamiento en apenas dos épocas (train acc sumamente cercana a 1), mientras que la precisión en el conjunto de validación tiende a oscilar entre 0.91 y 0.9375. La precisión sobre el conjunto de validación no ha mejorado desde la tercera época, por lo que podríamos haber detenido el entrenamiento de manera preventiva tras apenas 80 segundos.\n",
        "\n",
        "![Pretrained x20 loss function](./img_for_docs/pretrained_loss_resnet_x20.png)\n",
        "![Pretrained x20 acc function](./img_for_docs/pretrained_acc_resnet_x20.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con la gran mejora de rendimiento obtenida para el conjunto de datos pequeño, pasamos a ejecutar el proceso de entrenamiento y test sobre el conjunto de datos completo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "aJJ43le8KcFi"
      },
      "outputs": [],
      "source": [
        "# Reset model, optimizer and scheduler just in case\n",
        "model_pretrained = tv.models.resnet50(weights=tv.models.ResNet50_Weights.DEFAULT).to(DEVICE)\n",
        "new_optimizer = torch.optim.Adam(model_pretrained.parameters(), lr=1e-4)\n",
        "new_scheduler = torch.optim.lr_scheduler.ExponentialLR(new_optimizer, gamma=0.95)\n",
        "\n",
        "train_loss_pretrained_resnet_x200, train_acc_pretrained_resnet_x200, val_loss_pretrained_resnet_x200, val_acc_pretrained_resnet_x200 = list(), list(), list(), list()\n",
        "\n",
        "if TRAIN_PRETRAINED_MODELS and TRAIN_BIG_MODELS:\n",
        "  PRETRAINED_RESNET_X200_RESULTS = train_validate_model(model_pretrained, new_scheduler, new_optimizer, train_loader_x200, val_loader_x200, DATASET_200, 30, 'resnet50', True)\n",
        "  BEST_PRETRAINED_RESNET_X200_PATH = PRETRAINED_RESNET_X200_RESULTS[0]\n",
        "  train_loss_pretrained_resnet_x200, train_acc_pretrained_resnet_x200, val_loss_pretrained_resnet_x200, val_acc_pretrained_resnet_x200 = PRETRAINED_RESNET_X200_RESULTS[1:]\n",
        "if TEST_BIG_MODELS:\n",
        "  pretrained_x200_accuracy = test_model(model_pretrained, test_loader_x200, BEST_PRETRAINED_RESNET_X200_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_PRETRAINED_MODELS and TRAIN_BIG_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_pretrained_resnet_x200, val_loss_pretrained_resnet_x200)\n",
        "    graph_accuracy_function(train_acc_pretrained_resnet_x200, val_acc_pretrained_resnet_x200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podíamos anticipar, los resultados obtenidos son significativamente mejores a los obtenidos haciendo uso de un modelo entrenado desde cero. \n",
        "\n",
        "El mejor resultado obtenido permite al modelo clasificar las aves del conjunto de test con una precisión del 82.1%. El entrenamiento que ha dado lugar a estos pesos ha tenido como tamaño de lote 16 imágenes, y ha consistido en 30 épocas que han tardado en ejecutarse 64 minutos y 38 segundos. \n",
        "\n",
        "En este entrenamiento, no ha tenido lugar una mejora significativa de la precisión sobre el conjunto de validación a partir de la época 10, por lo que podríamos haber aplicado mecanismos de detención temprana de entrenamiento y haber concluido este en poco más de 20 minutos. Esto no ha sido así en todas las ejecuciones que hemos realizado, ya que en algunas de ellas se han obtenido ganancias cercanas al 5% más allá de las 20 épocas de entrenamiento.\n",
        "\n",
        "Para otras ejecuciones con tamaño de lote 24, tanto aplicando aumento de datos derivado de la modificación de brillo, contraste y saturación como sin hacerlo; se han obtenido precisiones sobre el conjunto de test que oscilan entre el 77 y 81%. En este sentido, no se presentan diferencias respecto de usar un tamaño de lote de 16. Los tiempos de ejecución no han sufrido cambios drásticos en función del tamaño de lote, oscilando entre los 62 y 66 minutos aproxima e indistintamente. \n",
        "\n",
        "\n",
        "![Pretrained x200 loss function](./img_for_docs/pretrained_loss_resnet_x200.png)\n",
        "![Pretrained x200 acc function](./img_for_docs/pretrained_acc_resnet_x200.png)\n",
        "\n",
        "Si observamos las gráficas con la evolución de la función de pérdida y la precisión, podemos ver como el modelo se ajusta perfectamente al conjunto de entrenamiento a partir de la época 9, mientras que la precisión en el conjunto de validación se estabiliza y mejora muy tímidamente a partir de la época 6. Los valores de pérdida siguen unas tendencias similares, no disminuyendo significativamente a partir de la época 10 para entrenamiento y 5 para validación.\n",
        "\n",
        "Sin embargo, no en todas las ejecuciones se ha seguido el comportamiento descrito en el párrafo anterior. En algunas ocasiones la precisión ya alcanzaba valores cercanos al 75% en la primera o segunda época, de manera que la mejora ha sido aún más rápida y el entrenamiento podría haberse detenido en apenas 6 épocas. En estos casos la precisión final sobre el conjunto de test era cercana al 80%. Análogamente, el descenso del valor de pérdida ha sido aún más pronunciado que en las gráficas mostradas.\n",
        "\n",
        "En una ocasión concreta, el modelo comenazaba en sus primeras épocas presentando unas precisiones sobre los conjuntos de entrenamiento y validación mucho más reducidas, de entorno al 30-40% durante las 6 primeras épocas. La mejora en la métrica era constante, pero el resultado final fue el peor de los ejecutados, con una precisión del 72%. Esta ejecución empleaba tamaño de lote 16 y todas las transformaciones descritas en el apartado de aumento de datos. No se consiguió replicar esta ejecución, proporcionando las demás valores de precisión en el rango 77-81%, salvo la destacada, que supera el 82%."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clasificador EfficientNet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otra arquitectura, alternativa a ResNet, que parece proporcionar muy buenos resultados según la documentación de PyTorch es EfficientNet. \n",
        "\n",
        "Esta familia de arquitecturas cuenta con 8 variantes (b0 a b7) en función de la potecia computacional y memoria requerida. En nuestro caso nos interesan especialmente la versión b3 y b4, pues son las primeras que proporcionan mejores resultados sobre el conjunto ImageNet que ResNet-50. \n",
        "\n",
        "Según la documentación de PyTorch, la versión b3 requiere 2.23 veces menos cálculos que ResNet-50, mientras que la versión b4 requiere aproximadamente el mismo número de cálculos. Por tanto, se decidió probar a emplear un clasificador basado en EfficientNet_b3. Sin embargo, a la hora de realizar el entrenamiento, la versión b3 necesita de unos 8 minutos por época para el conjunto de datos pequeño (teoricamente 10 veces más para el conjunto de datos completo). \n",
        "\n",
        "Dados los altos tiempos de entrenamiento, nos vemos a obligados a desistir y bajar a versiones \"inferiores\". Se realizaron pruebas con las versiones b2 y b1, pero finalmente tuvimos que usar la más básica, b0.\n",
        "\n",
        "En esta tanda de experimentos se decidió utilizar una versión alternativa del optimizador ADAM, llamada ADAMW, que calcula la función de decaimiento de los pesos de forma distinta. También se cambia el \"scheduler\" para que la tasa de aprendizaje se reduzca de forma multiplicataiva cada 4 épocas, en lugar de forma exponencial como se venía realizando hasta el momento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "BEST_EFFICIENTNET_X20_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_EfficientNet_Transfer_x20_ep=18_acc=0.9479166666666666.pt')\n",
        "BEST_EFFICIENTNET_X200_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_EfficientNet_Transfer_x200_ep=26_acc=0.7976973684210527.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_EFFICIENTNET_MODELS and TRAIN_SMALL_MODELS:\n",
        "    model_efficient_x20 = tv.models.efficientnet_b0(weights=tv.models.EfficientNet_B0_Weights.DEFAULT).to(DEVICE)\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2)\n",
        "    criterion.to(DEVICE)\n",
        "    efficient_optimizer = torch.optim.AdamW(model_efficient_x20.parameters(), lr=1e-3)\n",
        "    efficient_scheduler = torch.optim.lr_scheduler.StepLR(efficient_optimizer, step_size=4, gamma=0.96)\n",
        "    if EXECUTING_FOR_FIRST_TIME:\n",
        "        print(model_efficient_x20)\n",
        "        torchsummary.summary(model_efficient_x20, (3, 320, 320))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_EFFICIENTNET_MODELS and TRAIN_SMALL_MODELS:\n",
        "    train_loss_efficient_x20, train_acc_efficient_x20, val_loss_efficient_x20, val_acc_efficient_x20 = list(), list(), list(), list()\n",
        "\n",
        "    EFFICIENTNET_X20_RESULTS = train_validate_model(model_efficient_x20, efficient_scheduler, efficient_optimizer, train_loader_x20, val_loader_x20, DATASET_20, 30, 'efficientnet', True)\n",
        "    BEST_EFFICIENTNET_X20_PATH = EFFICIENTNET_X20_RESULTS[0]\n",
        "    train_loss_efficient_x20, train_acc_efficient_x20, val_loss_efficient_x20, val_acc_efficient_x20 = EFFICIENTNET_X20_RESULTS[1:]\n",
        "if TEST_SMALL_MODELS:\n",
        "    efficient_x20_accuracy = test_model(model_efficient_x20, test_loader_x20, BEST_EFFICIENTNET_X20_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_EFFICIENTNET_MODELS and TRAIN_SMALL_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_efficient_x20, val_loss_efficient_x20)\n",
        "    graph_accuracy_function(train_acc_efficient_x20, val_acc_efficient_x20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el mejor entrenamiento, con el ya habitual tamaño de lote 16, se ha conseguido una precisión sobre el conjunto pequeño del 88.3% tras 4 minutos y 57 segundos de entrenamiento. Estos resultados son un 5.7% peores que los obtenidos con ResNet-50. \n",
        "\n",
        "Respecto de la evolución de la función de pérdida y precisión podemos observar unas gráficas con valores alternos y oscilantes en un determinado rango para el conjunto de validación. En la mayoría de entrenamientos realizados con la arquitectura ResNet-50 estás gráficas eran mucho más suaves. Aparentemente seguimos sufriendo de \"overfitting\" en tanto que la pérdida para el conjunto de validación oscila, pero para el conjunto de entrenamiento tiende a descender. Dado que la precisión para validación fluctuaba decidimos no detener el entrenamiento de forma prematura; de hecho, los mejores resultados se consiguieron en la época 18, estando tentados de haberlo detenido cerca de la época 12.\n",
        "\n",
        "![Loss efficientnet x20](./img_for_docs/efficientnet_b0_loss_x20.png)\n",
        "![Acc efficientnet x200](./img_for_docs/efficientnet_b0_acc_x20.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_EFFICIENTNET_MODELS and TRAIN_BIG_MODELS:\n",
        "    model_efficient_x200 = tv.models.efficientnet_b0(weights=tv.models.EfficientNet_B0_Weights.IMAGENET1K_V1).to(DEVICE)\n",
        "    criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2)\n",
        "    criterion.to(DEVICE)\n",
        "    efficient_optimizer = torch.optim.AdamW(model_efficient_x200.parameters(), lr=1e-4)\n",
        "    efficient_scheduler = torch.optim.lr_scheduler.StepLR(efficient_optimizer, step_size=4, gamma=0.96)\n",
        "\n",
        "    train_loss_efficient_x200, train_acc_efficient_x200, val_loss_efficient_x200, val_acc_efficient_x200 = list(), list(), list(), list()\n",
        "\n",
        "    EFFICIENTNET_X200_RESULTS = train_validate_model(model_efficient_x200, efficient_scheduler, efficient_optimizer, train_loader_x200, val_loader_x200, DATASET_200, 30, 'efficientnet', True)\n",
        "    BEST_EFFICIENTNET_X200_PATH = EFFICIENTNET_X200_RESULTS[0]\n",
        "    train_loss_efficient_x200, train_acc_efficient_x200, val_loss_efficient_x200, val_acc_efficient_x200 = EFFICIENTNET_X200_RESULTS[1:]\n",
        "if TEST_BIG_MODELS:\n",
        "    efficient_x200_accuracy = test_model(model_efficient_x200, test_loader_x200, BEST_EFFICIENTNET_X200_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_EFFICIENTNET_MODELS and TRAIN_BIG_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_efficient_x200, val_loss_efficient_x200)\n",
        "    graph_accuracy_function(train_acc_efficient_x200, val_acc_efficient_x200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el mejor entrenamiento sobre el conjunto completo se ha obtenido una precisión del 78.4% tras 43 minutos y 10 segundos de entrenamiento. Estos resultados son un 3.7% peores que los obtenidos con ResNet-50. \n",
        "\n",
        "En esta ocasión, si nos fijamos en las gráficas adjuntas a continuación, podemos observar una tendencia mucho más suave, en línea con lo obtenido en experimentos con otros modelos. Volvemos a observar \"overfitting\" dada la continua disminución de la función de pérdida para el conjunto de entrenamiento mientras que se estanca para el conjunto de validación. Para la precisión, el modelo está cerca obtener precisiones perfectas para el conjunto de entrenamiento, mientras que para el conjunto de validación oscila entre el 74 y 78%.\n",
        "\n",
        "![Loss efficientnet x20](./img_for_docs/efficientnet_b0_loss_x200.png)\n",
        "![Acc efficientnet x200](./img_for_docs/efficientnet_b0_acc_x200.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clasificador MobileNetV3"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En los apartados anteriores conseguimos obtener clasificadores que funciona nrazonablemente bien, pero cuyo tiempo de entrenamiento y ejecución puede ser alto en función de si se entrena desde cero o no y el número de épocas empleadas.\n",
        "\n",
        "El disponer de un modelo de clasificación para aves puede resultar especialmente útil para investigadores o aficionados que deseen conocer más datos sobre la especie de pájaro que han observado en su último viaje. Sin embargo, no todo el mundo dispone de equipos pórtatiles, o incluso de sobremesa, con GPU y los requisitos para ejecutarlos. Si pensamos en el aficionado o investigador que va al monte y ha fotografiado un pájaro desconocido, es lógico pensar que querría usar su teléfono móvil para obtener más información sobre dicho pájaro.\n",
        "\n",
        "La aplicación con la que realizar esta consulta podría mandar la foto a un servidor que ejecute el clasificador y devolver el resultado, pero no siempre disponemos de cobertura a la red de datos móviles en entornos rurales, por lo que convendría realizar estos cálculos localmente en el dispostivo. Es en estas situaciones donde convendría emplear modelos menos costosos en tiempo de procesamiento y memoria, aunque se realice un pequeño sacrificio en la precisión de los resultados. Así pues, entran al tablero de juego modelos ligeros como MobileNetV3.\n",
        "\n",
        "MobileNetV3 es una arquitectura de red neuronal convolucional (CNN) diseñada específicamente para su implementación en dispositivos móviles y aplicaciones de visión por computadora en tiempo real. Fue propuesto por investigadores de Google en su artículo \"Searching for MobileNetV3\" en 2019. El objetivo principal de este modelo es lograr un equilibrio entre la precisión y la eficiencia computacional, es decir, obtener modelos de alta calidad que sean lo más livianos y rápidos posible.\n",
        "\n",
        "Dados los pésimos resultados que obtuvimos al intentar entrenar ResNet-50 desde cero, en este apartado abordaremos únicamente el proceso de transferencia de aprendizaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mobilenet_x20 = tv.models.mobilenet_v3_large(weights=tv.models.MobileNet_V3_Large_Weights.DEFAULT).to(DEVICE)\n",
        "mobilenet_optimizer = torch.optim.Adam(model_mobilenet_x20.parameters(), lr=1e-3)\n",
        "mobilenet_scheduler = torch.optim.lr_scheduler.ExponentialLR(mobilenet_optimizer, gamma=0.95)\n",
        "\n",
        "if EXECUTING_FOR_FIRST_TIME:\n",
        "    print(model_mobilenet_x20)\n",
        "    torchsummary.summary(model_mobilenet_x20, (3, 224, 224))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La creación del modelo y su optimizador y controlador de la tasa de aprendizaje asociado es análoga a la empleada anteriormente para ResNet, simplemente debemos cambiar la clase a instanciar. PyTorch dispone de dos variantes de la red, en función de los requisitos computacionales y de memoria. Para nuestros experimentos decidimos utilizar la mayor de las dos, pues a pesar de tener el triple coste computacional obtiene un 9% más de precisión sobre el conjunto de datos de prueba de ImageNet. Esta versión tiene 5.483.032 parámetros entrenables (unas 4.3 veces menos que ResNet-50) y requiere de 126.90 MB (unas 7.15 veces menos que ResNet-50)\n",
        "\n",
        "Respecto del modelo ResNet-50 que hemos empleado anteriormente, la documentación de PyTorch indica que MobileNetV3-Large requiere de 18.5 veces menos operaciones por segundo aproximadamente. No obtendremos una ganancia proporcional en tiempo de ejecución, pero si que hemos observado una reducción en el tiempo de ejecución cercana a un factor 2.25.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "BEST_MOBILENET_X20_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_MobileNet_Transfer_x20_ep=10_acc=0.8697916666666666.pt')  \n",
        "BEST_MOBILENET_X200_PATH = os.path.join(os.path.join(os.getcwd(), 'results'), 'model_MobileNet_Transfer_x200_ep=30_acc=0.7582236842105263.pt') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loss_mobilenet_x20, train_acc_mobilenet_x20, val_loss_mobilenet_x20, val_acc_mobilenet_x20 = list(), list(), list(), list()\n",
        "\n",
        "if TRAIN_MOBILENET_MODELS and TRAIN_SMALL_MODELS:\n",
        "  MOBILENET_X20_RESULTS = train_validate_model(model_mobilenet_x20, mobilenet_scheduler, mobilenet_optimizer, train_loader_x20, val_loader_x20, DATASET_20, 40, 'mobilenet', True)\n",
        "  BEST_MOBILENET_X20_PATH = MOBILENET_X20_RESULTS[0]\n",
        "  train_loss_mobilenet_x20, train_acc_mobilenet_x20, val_loss_mobilenet_x20, val_acc_mobilenet_x20 = MOBILENET_X20_RESULTS[1:]\n",
        "if TEST_SMALL_MODELS:\n",
        "  mobilenet_x20_accuracy = test_model(model_mobilenet_x20, test_loader_x20, BEST_MOBILENET_X20_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MOBILENET_MODELS and TRAIN_SMALL_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_mobilenet_x20, val_loss_mobilenet_x20)\n",
        "    graph_accuracy_function(train_acc_mobilenet_x20, val_acc_mobilenet_x20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que la red es en principio más sencilla y está pensada para usarse en dispositivos móviles en tiempo real, los resultados son algo peores que los obtenidos con RESNET50 en el apartado anterior.\n",
        "\n",
        "El mejor entrenamiento realizado permite generar un modelo con el que clasificar a las aves del conjunto de prueba pequeño correctamente un 89.5%. El entrenamiento en cuestión tenía como tamaño de lote 16 imágenes, se ha llevado a cabo durante 40 épocas y ha tardado en ejecutarse 4 minutos y 9 segundos. Distintas combinaciones de entrenamientos con tamaño de lote 24, más épocas y distintos parametros LR y gamma del optimizador y \"scheduler\" proporcionan resultados relativamente similares, siendo la menor de las precisiones obtenidas del 85.7%.\n",
        "\n",
        "Nótese que la diferencia de precision respecto del modelo RESNET50 para el conjunto pequeño es de apenas un 4%, dentro de un rango de precisiones muy altas. Por su parte, el tiempo de entrenamiento ha sido menor incluso ejecutando un mayor número de épocas.\n",
        "\n",
        "En lo que respecta a las curvas de la función de pérdida y precisión, podemos ver en las siguientes gráficas como salvo para el caso de la función de pérdida en validación no se ha experimentado un cambio significativo durante el transcurso del entrenamiento. Como en ocasiones anteriores, podemos sospechar de sufrir \"overfitting\" al tenerse precisiones en entrenamiento cercanas a 1 desde épocas tempranas. \n",
        "\n",
        "![Mobilenet loss x20](./img_for_docs/mobilenet_loss_x20.png)\n",
        "![Mobilenet acc x20](./img_for_docs/mobilenet_acc_x20.png)\n",
        "\n",
        "Sobre el conjunto de validación no se obtuvieron mejoras en la precisión desde la época 10, por lo que se podría haber detenido tempranamente el entrenamiento y haber empleado solamente unos 60 segundos en realizarlo, sin presumiblemente existir mucha diferencia en los resultados finales.\n",
        "\n",
        "No en todas las ejecuciones se ha observado este ajuste tan rápido al conjunto de entrenamiento y validación; en alguna de ellas se ha comenzado con una precisión sobre el conjunto de validación cercana al 40%, que fue mejorando y alcanzó niveles cercanos a los mostrados en torno a la época 12."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_mobilenet_x200 = tv.models.mobilenet_v3_large(weights=tv.models.MobileNet_V3_Large_Weights.DEFAULT).to(DEVICE)\n",
        "mobilenet_optimizer = torch.optim.Adam(model_mobilenet_x200.parameters(), lr=1e-4)\n",
        "mobilenet_scheduler = torch.optim.lr_scheduler.ExponentialLR(mobilenet_optimizer, gamma=0.90)\n",
        "\n",
        "train_loss_mobilenet_x200, train_acc_mobilenet_x200, val_loss_mobilenet_x200, val_acc_mobilenet_x200 = list(), list(), list(), list()\n",
        "\n",
        "if TRAIN_MOBILENET_MODELS and TRAIN_SMALL_MODELS:\n",
        "  MOBILENET_X200_RESULTS = train_validate_model(model_mobilenet_x200, mobilenet_scheduler, mobilenet_optimizer, train_loader_x200, val_loader_x200, DATASET_200, 30, 'mobilenet', True)\n",
        "  BEST_MOBILENET_X200_PATH = MOBILENET_X200_RESULTS[0]\n",
        "  train_loss_mobilenet_x200, train_acc_mobilenet_x200, val_loss_mobilenet_x200, val_acc_mobilenet_x200 = MOBILENET_X200_RESULTS[1:]\n",
        "if TEST_SMALL_MODELS:\n",
        "  mobilenet_x200_accuracy = test_model(model_mobilenet_x200, test_loader_x200, BEST_MOBILENET_X200_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TRAIN_MOBILENET_MODELS and TRAIN_BIG_MODELS and GENERATE_GRAPHS:\n",
        "    graph_loss_function(train_loss_mobilenet_x200, val_loss_mobilenet_x200)\n",
        "    graph_accuracy_function(train_acc_mobilenet_x200, val_acc_mobilenet_x200)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras 27 minutos y 55 segundos obtenemos los mejores resultados para un entrenamiento del modelo MobileNetV3 sobre el conjunto de datos completo. En este caso, la precisión desciende hasta el 74.8%, siendo esta un 7.3% menor que la obtenida por el procedimiento equivalente que hace uso de ResNet-50. A cambio, podemos destacar que el tiempo de ejecución fue unas 2.5 veces menor. \n",
        "\n",
        "Si observamos la evolución de la función de pérdida podemos intuir la existencia de \"overfitting\" a partir de la sexta o séptima época dado que la pérdida para el conjunto de entrenamiento sigue reduciendose mientras que la de validación comienza a oscilar en un rango pequeño acotado. \n",
        "\n",
        "En la gráfica de la precisión podemos ver como en esta ocasión el modelo ha tardado varias épocas en alcanzar una precisión aceptable y dejar de mejorarla. En cualquier caso, se produce un ajuste casi perfecto al conjunto de entrenamiento una vez se alcanzan las 15 épocas. La mejor precisión sobre el conjunto de validación se produce justamente en la última época, mejorando en 0.66% el resultado anterior. Dudamos seriamente que emplear un mayor número de épocas hubiese acabado proporcionando mejores resultados, pues en ejecuciones de 40 y 50 épocas se obtuvieron resultados ligeramente peores y en esta ejecución concreta se llevaba manteniendo la tendencia durante un número considerable de épocas.\n",
        "\n",
        "El comportamiento expuesto en estas gráficas recuerda al explicado en el apartado anterior para el modelo ResNet-50.\n",
        "\n",
        "![Mobilenet loss x200](./img_for_docs/mobilenet_loss_x200.png)\n",
        "![Mobilenet acc x200](./img_for_docs/mobilenet_acc_x200.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gráfica comparativa entre modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ht46OVyhGgNW"
      },
      "outputs": [],
      "source": [
        "# Values are hardcoded as multiple test have been executed for each model, we take the best from each\n",
        "if GENERATE_GRAPHS:\n",
        "  logs = {}\n",
        "  logs['Setups'] = ['Baseline ResNet', 'Transfer ResNet', 'Transfer MobileNetV3', 'Transfer EfficientNet_B0']\n",
        "  logs['Accuracy'] = [0.47, 0.9375, 0.895, 0.883]\n",
        "  logs['Time'] = [16.5, 6.85, 4.33, 4.95] # Extrapolated to decimal base\n",
        "\n",
        "  cmap = mpl.colormaps['Blues_r']\n",
        "  colors = [cmap(0.9), cmap(0.5), cmap(0.1)]\n",
        "  x_min, x_max = np.min(logs['Accuracy']), np.max(logs['Accuracy'])\n",
        "\n",
        "  plt.barh(logs['Setups'], logs['Accuracy'], color=colors)\n",
        "  plt.xlim(x_min - 5e-2, x_max + 5e-2)\n",
        "  plt.xlabel('Accuracy (%)')\n",
        "  plt.ylabel('Setups')\n",
        "  plt.show()\n",
        "\n",
        "  plt.barh(logs['Setups'], logs['Time'], color=colors)\n",
        "  plt.xlim(3.8, 17)\n",
        "  plt.xlabel('Execution Time (minutes)')\n",
        "  plt.ylabel('Setups')\n",
        "  plt.show()\n",
        "\n",
        "  logs2 = {}\n",
        "  logs2['Setups'] = ['Baseline ResNet', 'Transfer ResNet', 'Transfer MobileNetV3', 'Transfer EfficientNet_B0']\n",
        "  logs2['Accuracy'] = [0.2335, 0.821, 0.748, 0.784]\n",
        "  logs2['Time'] = [40.37, 64.66, 27.92, 44.17] # Extrapolated to decimal base\n",
        "\n",
        "  cmap = mpl.colormaps['Blues_r']\n",
        "  colors = [cmap(0.9), cmap(0.5), cmap(0.1)]\n",
        "  x_min, x_max = np.min(logs2['Accuracy']), np.max(logs2['Accuracy'])\n",
        "\n",
        "  plt.barh(logs2['Setups'], logs2['Accuracy'], color=colors)\n",
        "  plt.xlim(x_min - 5e-2, x_max + 5e-2)\n",
        "  plt.xlabel('Accuracy (%)')\n",
        "  plt.ylabel('Setups')\n",
        "  plt.show()\n",
        "\n",
        "  plt.barh(logs2['Setups'], logs2['Time'], color=colors)\n",
        "  plt.xlim(25, 67)\n",
        "  plt.xlabel('Execution Time (minutes)')\n",
        "  plt.ylabel('Setups')\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En las siguientes gráficas se puede observar una comparativa directa entre la precisión obtenida para cada modelo y el tiempo de entrenamiento que ha requerido para los conjuntos de datos con 20 clases (izquierda) y 200 clases (derecha). Téngase en cuenta que todos los entrenamientos constan de 30 épocas, a excepción de \"Baseline ResNet\" para el conjunto de datos pequeño, que requirió de 70 épocas ya que nos dimos cuenta que la mejora continuaba más allá de las 30 épocas y esta era significativa; y el entrenamiento de MobileNet para el conjunto pequeño, que consta de 40 épocas.\n",
        "\n",
        "![Acc x20 comparativa](./img_for_docs/comparativa_acc_x20.png)\n",
        "![Acc x200 comparativa](./img_for_docs/comparativa_acc_x200.png)\n",
        "![Training time x20 comparativa](./img_for_docs/execution_time_x20.png)\n",
        "![Training time x200 comparativa](./img_for_docs/execution_time_x200.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obviando los desastrosos resultados obtenidos al intentar entrenar un clasificador desde cero, podemos ver como ResNet-50 proporciona buenas precisiones para ambos conjuntos tras ser entrenado en un tiempo relativamente razonable. Así también, podemos ver como MobileNetV3 ofrece resultados algo peores, pero que están dentro de los márgenes de lo que podríamos considerar aceptable, todo ello requeriendo un tiempo de entrenamiento significativamente menor. Cabe también destacar que los tiempos de inferenica de MobileNetV3 son menores que los empleados por ResNet-50. \n",
        "\n",
        "En lo relativo a EfficientNet_b0, esta obtiene un peor desempeño sobre el conjunto de datos pequeño que el resto de modelos; pero para el conjunto completo consigue superar ligeramente a MobileNetV3. Nótese que el tiempo de ejecución necesario para su entrenamiento se encuentra entre los propios de los otros modelos. Según la documentación de PyTorch necesita del triple de operaciones que MobileNetV3, por lo que podría no ser apto para dispositivos móviles. Sin embargo, si que podría ser útil fuera de este ámbito, ya que teóricamente requiere de 5 veces menos operaciones que ResNet-50 y ofrece resultados bastante razonables con tiempos de entrenamiento entre 2 y 3 veces menores.\n",
        "\n",
        "Todos los modelos obtienen sobre nuestro conjunto de datos precisiones algo mayores que las obtenidas en sus entrenamientos originales sobre ImageNet (de acuerdo a la documentación de PyTorch).\n",
        "\n",
        "Como se mencionó en apartados anteriores, se obvia EfficientNet_b3 y b4 de la comparativa al no haber podido terminar los entrenamientos correspondientes. De seguir la tendencia de mejora que hemos venido observando entre los pesos originales sobre ImageNet y los nuestros sobre CUB 200-2011, estimamos que podríamos obtener entre un 2% y 4% de mejora en la precisión de las predicciones sobre el conjunto de datos con 200 clases. Dado el elevadísimo tiempo de ejecución que sería necesario (estimamos varias horas para el conjunto completo a partir de lo que ha tardado para una época del conjunto pequeño); decidimos que no merece la pena un sacrificio en tiempo tan grande por una ligera mejora en precisión, máxime una vez hemos aceptado niveles aceptables con ResNet-50 y algo menores con la variante más \"básica\" de la familia EfficientNet."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusión"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En esta práctica se ha abordado el problema de la creación de un modelo capaz de clasificar los pájaros del conjunto de datos CUB 200-2011, el cual presenta 200 clases distintas de pájaros y pocas muestras para cada clase (unas 30), lo que dificulta la tarea.\n",
        "\n",
        "Tras la realización de la práctica, y en virtud de los experimentos y resultados obtenidos, podemos concluir lo siguiente:\n",
        "\n",
        "- Dado el escaso número de muestras por clase resulta inviable crear un modelo desde cero con el que conseguir una buena generalización y predicciones acertadas, a pesar de emplear técnicas de aumento de datos. La precisión para el conjunto de datos pequeño haciendo uso de ResNet-50 apenas llega al 47%, mientras que para el conjunto de datos completo no se ha conseguido obtener una precisión superior al 23.5%.\n",
        "- A la luz de los resultados expuestos en el punto anterior, nos vemos obligados a partir de pesos pre-entrenados para conjuntos de datos mucho más grandes, como ImageNet, y realizar un proceso de transferencia de aprendizaje con el que dotar al modelo de cierto conocimiento sobre nuestro conjunto de datos y problemática concreta.\n",
        "- El uso de transferencia de aprendizaje permite reducir considerablemente el tiempo de entrenamiento y permite obtener unos resultados mucho mejores. Para la arquitectura ResNet-50 conseguimos una precisión del 94% sobre el conjunto de datos pequeño y de 82.1% sobre el conjunto de datos completo.\n",
        "- Resulta esencial elegir una arquitectura base de acuerdo a ciertos criterios predefinidos. En nuestro caso, se eligió ResNet-50 por ser la que mejor relación entre coste computacional y precisión parecía proporcionar de entre aquellas disponibles en PyTorch que pudimos llegar a ejecutar. Somos conscientes de que con arquitecturas más complejas como EfficientNet (a partir de la versión B3) o incluso EfficientNetV2 podríamos obtener mejores resultados (estimamos que entre un 2 y 4% más de precisión sobre el conjunto de datos completo si usaramos la versión B4), pero hemos sido incapaces de completar entrenamientos con estas versiones dados los altos tiempos de ejecución requeridos. La versión básica de EfficientNet proporciona peores resultados que ResNet-50, pero con tiempos de entrenamiento menores.\n",
        "- En el ámbito de arquitecturas aptas para dispositivos móviles y tiempo real, decidimos probar a ajustar un modelo MobileNetV3 entrenado sobre el conjunto de datos de ImageNet. Los resultados obtenidos son ligeramente peores que los proporcionados por ResNet-50 (4% para el conjunto de datos pequeño y 7.4% para el conjunto completo); pero el tiempo de entrenamiento es significativamente menor, ídem para el tiempo de inferencia."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bibliografía"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Durante la realización de esta práctica se ha consultado las siguientes fuentes de información:\n",
        "- [Bird by Bird using Deep Learning](https://towardsdatascience.com/bird-by-bird-using-deep-learning-4c0fa81365d7)\n",
        "- [Bird by Bird using Deep Learning repository](https://github.com/slipnitskaya/caltech-birds-advanced-classification)\n",
        "- [Image Classification of birds](https://github.com/ecm200/caltech_birds)\n",
        "- [pytorch-cubbirds200-classification](https://www.kaggle.com/code/sharansmenon/pytorch-cubbirds200-classification/notebook)\n",
        "- [A Visual Guide to Learning Rate Schedulers in PyTorch](https://towardsdatascience.com/a-visual-guide-to-learning-rate-schedulers-in-pytorch-24bbb262c863)\n",
        "- [Image classification: ResNet vs EfficientNet vs EfficientNet_v2 vs Compact Convolutional Transformers](https://medium.com/@enrico.randellini/image-classification-resnet-vs-efficientnet-vs-efficientnet-v2-vs-compact-convolutional-c205838bbf49)\n",
        "- [Documentación de PyTorch](https://pytorch.org/vision/main/index.html)\n",
        "    - [MODELS AND PRE-TRAINED WEIGHTS](https://pytorch.org/vision/main/models.html)\n",
        "        - [RESNET50](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html#torchvision.models.resnet50)\n",
        "        - [MOBILENET_v3_LARGE](https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v3_large.html#torchvision.models.mobilenet_v3_large)\n",
        "        - [EFFICIENTNET](https://pytorch.org/vision/main/models/efficientnet.html)\n",
        "    - [FINETUNING TORCHVISION MODELS](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)\n",
        "- [Documentación de Matplotlib](https://matplotlib.org/stable/index.html)\n",
        "- [Documentación de Scikit-Learn](https://scikit-learn.org/stable/modules/classes.html#)\n",
        "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
        "- [Searching for MobileNetV3](https://arxiv.org/abs/1905.02244)\n",
        "- Apuntes de la asignatura \"Sistemas Inteligentes para la Gestión en la Empresa\" del Máster en Ingeniería Informática de la Universidad de Granada\n",
        "- Apuntes de la asignatura \"Inteligencia Computacional\" del Máster en Ingeniería Informática de la Universidad de Granada"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
